{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit6f19452678144e6e97437b4df42ffca5",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n0    1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n1    1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n2    1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n3    1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n4    1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n..  .. ..      ...       ...      ...      ...      ...       ...      ...   \n345  1  0  0.83508   0.08298  0.73739 -0.14706  0.84349  -0.05567  0.90441   \n346  1  0  0.95113   0.00419  0.95183 -0.02723  0.93438  -0.01920  0.94590   \n347  1  0  0.94701  -0.00034  0.93207 -0.03227  0.95177  -0.03431  0.95584   \n348  1  0  0.90608  -0.01657  0.98122 -0.01989  0.95691  -0.03646  0.85746   \n349  1  0  0.84710   0.13533  0.73638 -0.06151  0.87873   0.08260  0.88928   \n\n     0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n0   -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n1    0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n2    0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n3   -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n4    0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n..       ...  ...       ...      ...       ...      ...       ...      ...   \n345 -0.04622  ...  -0.04202  0.83479   0.00123  1.00000   0.12815  0.86660   \n346  0.01606  ...   0.01361  0.93522   0.04925  0.93159   0.08168  0.94066   \n347  0.02446  ...   0.03193  0.92489   0.02542  0.92120   0.02242  0.92459   \n348  0.00110  ...  -0.02099  0.89147  -0.07760  0.82983  -0.17238  0.96022   \n349 -0.09139  ...  -0.15114  0.81147  -0.04822  0.78207  -0.00703  0.75747   \n\n     -0.54487  0.18641  -0.45300  g  \n0    -0.06288 -0.13738  -0.02447  b  \n1    -0.24180  0.56045  -0.38238  g  \n2     1.00000 -0.32382   1.00000  b  \n3    -0.59573 -0.04608  -0.65697  g  \n4     0.00000 -0.00039   0.12011  b  \n..        ...      ...       ... ..  \n345  -0.10714  0.90546  -0.04307  g  \n346  -0.00035  0.91483   0.04712  g  \n347   0.00442  0.92697  -0.00577  g  \n348  -0.03757  0.87403  -0.16243  g  \n349  -0.06678  0.85764  -0.06151  g  \n\n[350 rows x 35 columns]\n"
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'ionosphere-data.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('ionosphere-data.csv', sep=',')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g']\n"
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy\n",
    "#df.to_numpy()\n",
    "temp = df.values\n",
    "R,C = temp.shape\n",
    "#print(C)\n",
    "\n",
    "# Split array into design matrix and labels\n",
    "ionosphere_labels = temp[:, C-1]\n",
    "print(ionosphere_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1 0 1.0 ... -0.06287999999999999 -0.13738 -0.02447]\n [1 0 1.0 ... -0.2418 0.56045 -0.38238]\n [1 0 1.0 ... 1.0 -0.32382 1.0]\n ...\n [1 0 0.94701 ... 0.00442 0.9269700000000001 -0.00577]\n [1 0 0.9060799999999999 ... -0.03757 0.87403 -0.16243]\n [1 0 0.8471 ... -0.06677999999999999 0.85764 -0.06151]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Remove labels to get design matrix\n",
    "ionosphere_design_matrix= np.delete(temp, C-1, 1)\n",
    "print(ionosphere_design_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0\n 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1\n 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0\n 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1]\n[0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1]\n"
    }
   ],
   "source": [
    "import Models.kfold_CV_try as cv\n",
    "\n",
    "# Train test split\n",
    "xTrain_ion, xTest_ion, yTrain_ion, yTest_ion = cv.split_train_test(ionosphere_design_matrix, ionosphere_labels, 0.2)\n",
    "# print(xTest_ion); print(xTrain_ion); print(yTest_ion);print(yTrain_ion)\n",
    "# change the binary label 'g' and 'b' into 1 and 0\n",
    "for i in enumerate(yTrain_ion):\n",
    "    if i[1] == 'g':\n",
    "        yTrain_ion[i[0]] = 1\n",
    "    elif i[1] == 'b':\n",
    "        yTrain_ion[i[0]] = 0\n",
    "yTrain_ion=yTrain_ion.astype('int')\n",
    "for i in enumerate(yTest_ion):\n",
    "    if i[1] == 'g':\n",
    "        yTest_ion[i[0]] = 1\n",
    "    elif i[1] == 'b':\n",
    "        yTest_ion[i[0]] = 0        \n",
    "yTest_ion=yTest_ion.astype('int')\n",
    "\n",
    "print(yTrain_ion);print(yTest_ion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1 0 1.0 ... -0.75761 1.0 -0.8443700000000001]\n [1 0 0.4709 ... -0.24339 0.2672 0.04233]\n [0 0 1.0 ... 1.0 1.0 1.0]\n ...\n [1 0 0.9882200000000001 ... 0.44195 -0.33483 0.37465]\n [1 0 0.85209 ... 0.66315 -0.11215 0.64933]\n [1 0 -1.0 ... -1.0 0.47568999999999995 1.0]] [1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0]\n(56, 34) (56,)\n"
    }
   ],
   "source": [
    "folds = 5 # delete folds later when embedded in function input\n",
    "# cv_train_data_ion is xTrain_ion split into five chunks\n",
    "dataset_split_in, cv_train_data_ion,cv_train_label_ion= cv.kfold_cross_validation(xTrain_ion,yTrain_ion,folds)\n",
    "# print(cv_train_data_ion,cv_train_label_ion)\n",
    "\n",
    "# the last input for cv.train_validation_split is the number of experiments you are running currently, 5 in total. \n",
    "# each experiments is organizing the five chunks from cv_train_data_ion into 4 chunks for training_data_ion and 1 chunk for validate_data_ion for cross validation, just need to uncomment the line you want to experiment currently. \n",
    "\n",
    "# will be in a for-loop when computing accuracy scores \n",
    "#exp1: \n",
    "validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,1)\n",
    "\n",
    "#exp2:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,2)\n",
    "\n",
    "#exp3:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,3)\n",
    "\n",
    "#exp4:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,4)\n",
    "\n",
    "#exp5: \n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,5)\n",
    "print(validate_data_ion,validate_labels_ion)\n",
    "print(validate_data_ion.shape,validate_labels_ion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Now fitting Ionosphere\n0.6278502593645168\n0.6105101112075536\n0.5937531080012328\n0.577571837975824\n0.5619568945910018\n0.5468972143408747\n0.5323803837486893\n0.5183929153039599\n0.5049204929595518\n0.49194818844277877\n0.4794606500771546\n0.4674422660941996\n0.45587730456934866\n0.4447500321710111\n0.4340448138918099\n0.42374619585666695\n0.4138389731906837\n0.4043082447942185\n0.3951394567237922\n0.3863184357234486\n0.3778314142979265\n0.36966504857060845\n0.36180643002860136\n0.35424309212620003\n0.3469630125973997\n0.33995461221839957\n0.3332067506621361\n0.3267087199984115\n0.320450236314618\n0.314421429862648\n0.30861283407666407\n0.30301537375311977\n0.297620352638085\n0.2924194406267521\n0.2874046607453286\n0.28256837605569285\n0.27790327659763525\n0.2734023664616731\n0.26905895106686817\n0.2648666247023271\n0.2608192583777768\n0.256910988017437\n0.25313620302205175\n0.24948953521615974\n0.24596584819122796\n0.2425602270499745\n0.23926796855286445\n0.2360845716642682\n0.2330057284929582\n0.23002731561940976\n0.22714538580065452\n0.22435616004213224\n0.22165602002503462\n0.21904150087696772\n0.21650928427333613\n0.21405619185661556\n0.21167917896061303\n0.20937532862686653\n0.207141845900496\n0.2049760523930598\n0.20287538110026623\n0.20083737146274777\n0.1988596646584825\n0.19693999911585888\n0.19507620623680205\n0.19326620631981198\n0.19150800467319606\n0.18979968790921353\n0.18813942041027074\n0.186525440958728\n0.18495605952228114\n0.18342965418727683\n0.18194466823269917\n0.1804996073379318\n0.1790930369177474\n0.17772357957831444\n0.17638991268832785\n0.17509076605967538\n0.1738249197323407\n0.1725912018585205\n0.17138848668118847\n0.17021569260259312\n0.16907178033840267\n0.16795575115343717\n0.16686664517513305\n0.16580353978108567\n0.16476554805720078\n0.16375181732316244\n0.16276152772209138\n0.16179389087142776\n0.16084814857221702\n0.1599235715741211\n0.1590194583936096\n0.15813513418291142\n0.15726994964742316\n0.1564232800093894\n0.155594524015768\n0.15478310298830128\n0.15398845991390442\n0.15321005857357547\n0.1524473827081145\n0.1516999352190216\n0.1509672374030211\n0.15024882821872865\n0.14954426358405223\n0.14885311570297677\n0.14817497242045233\n0.14750943660415722\n0.14685612555196867\n0.14621467042402547\n0.1455847156983171\n0.1449659186487828\n0.14435794884495035\n0.14376048767218733\n0.14317322787167924\n0.14259587309928923\n0.14202813750249152\n0.14146974531460593\n0.14092043046559663\n0.14037993620872974\n0.13984801476241576\n0.13932442696659364\n0.13880894195303917\n0.13830133682901083\n0.13780139637366892\n0.13730891274673027\n0.13682368520884372\n0.13634551985319412\n0.13587422934786395\n0.13540963268850212\n0.13495155496086947\n0.13449982711284794\n0.13405428573552072\n0.1336147728529438\n0.13318113572024995\n0.1327532266297385\n0.13233090272462075\n0.1319140258201043\n0.13150246223151366\n0.13109608260915742\n0.130694761779664\n0.13029837859352053\n0.1299068157785613\n0.1295199597991611\n0.12913770072090103\n0.12875993208048347\n0.12838655076068287\n0.12801745687012633\n0.12765255362771\n0.12729174725146172\n0.12693494685167037\n0.12658206432811145\n0.1262330142712009\n0.12588771386692177\n0.12554608280537052\n0.12520804319277917\n0.12487351946687257\n0.12454243831542959\n0.12421472859791888\n0.12389032127008735\n0.12356914931138485\n0.12325114765511111\n0.12293625312117827\n0.12262440435138602\n0.12231554174710865\n0.12200960740930072\n0.12170654508072967\n0.1214063000903478\n0.12110881929972009\n0.12081405105142802\n0.12052194511937127\n0.12023245266089454\n0.11994552617066796\n0.11966111943625356\n0.11937918749529172\n0.11909968659424608\n0.1188225741486463\n0.11854780870477087\n0.11827534990271507\n0.11800515844079082\n0.11773719604120726\n0.11747142541698367\n0.11720781024004732\n0.11694631511047099\n0.11668690552680788\n0.11642954785748123\n0.11617420931318924\n0.11592085792028696\n0.11566946249510844\n0.11541999261919353\n0.11517241861538562\n0.11492671152476747\n0.11468284308440381\n0.11444078570586123\n0.11420051245447527\n0.11396199702933782\n0.113725213743978\n0.11349013750771039\n0.11325674380762654\n0.11302500869120528\n0.11279490874952014\n0.11256642110102069\n0.11233952337586774\n0.11211419370080145\n0.11189041068452357\n0.1116681534035741\n0.11144740138868552\n0.11122813461159611\n0.1110103334723067\n0.11079397878676409\n0.11057905177495603\n0.11036553404940296\n0.11015340760403218\n0.10994265480342048\n0.10973325837239255\n0.10952520138596183\n0.10931846725960208\n0.10911303973983769\n0.10890890289514103\n0.10870604110712673\n0.10850443906203165\n0.10830408174247086\n0.10810495441945964\n0.10790704264469214\n0.107710332243068\n0.10751480930545779\n0.10732046018169929\n0.10712727147381614\n0.1069352300294514\n0.1067443229355088\n0.10655453751199327\n0.10636586130604594\n0.10617828208616441\n0.10599178783660382\n0.10580636675195186\n0.1056220072318711\n0.105438697876004\n0.10525642747903433\n0.1050751850258996\n0.10489495968714992\n0.10471574081444779\n0.1045375179362041\n0.10436028075334593\n0.10418401913521162\n0.10400872311556869\n0.10383438288875066\n0.10366098880590843\n0.10348853137137311\n0.10331700123912546\n0.1031463892093696\n0.1029766862252066\n0.10280788336940494\n0.10263997186126489\n0.10247294305357323\n0.10230678842964559\n0.10214149960045393\n0.10197706830183499\n0.10181348639177909\n0.10165074584779485\n0.10148883876434835\n0.1013277573503739\n0.10116749392685445\n0.10100804092446901\n0.100849390881305\n0.10069153644063358\n0.10053447034874623\n0.10037818545284971\n0.10022267469901822\n0.10006793113020132\n0.09991394788428468\n"
    }
   ],
   "source": [
    "import Models.logisticRegression as lr\n",
    "\n",
    "ionslr1 = lr.Logistic_Regression(0.02,\"Ionosphere\",\"binary\") # input step size\n",
    "# params = ionslr1.fit(cv_train_data, cv_train_label, 0.02, 1e-1)\n",
    "params1 = ionslr1.fit(training_data_ion,training_labels_ion,0.02,1e-1)\n",
    "#adding a separate comment\n",
    "# input learning rate and termination conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0]\n[1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0]\n"
    }
   ],
   "source": [
    "# %% Making LR prediction on the validatoin set\n",
    "\n",
    "# need to convert predictions into 1 or 0\n",
    "\n",
    "# pred_vali1 is a boolean array, pred_vali*1 gives binary 1 or 0 automatically \n",
    "pred_vali = ionslr1.predict(params1,validate_data_ion)\n",
    "pred_vali=pred_vali * 1\n",
    "\n",
    "print(pred_vali)\n",
    "print(validate_labels_ion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
    }
   ],
   "source": [
    "# try prediction for testing set \n",
    "# print(xTest_ion); print(yTest_ion)\n",
    "pred_test = ionslr1.predict(params1,xTest_ion)\n",
    "# change boolean into 1 or 0\n",
    "pred_test = pred_test * 1\n",
    "print(pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "46886605\n0.10668284323551264\n0.10648338581962444\n0.10628475980202126\n0.10608696078954685\n0.1058899844148844\n0.10569382633637386\n0.10549848223783136\n0.10530394782837031\n0.10511021884222421\n0.10491729103857167\n0.10472516020136267\n0.10453382213914697\n0.10434327268490383\n0.10415350769587393\n0.1039645230533926\n0.10377631466272455\n0.10358887845290066\n0.10340221037655607\n0.1032163064097698\n0.10303116255190616\n0.10284677482545738\n0.10266313927588805\n0.10248025197148064\n0.10229810900318308\n0.10211670648445721\n0.10193604055112876\n0.10175610736123894\n0.10157690309489761\n0.10139842395413694\n0.10122066616276733\n0.10104362596623415\n0.10086729963147613\n0.10069168344678463\n0.10051677372166443\n0.10034256678669576\n0.10016905899339752\n0.09999624671409177\nNow fitting Ionosphere\n0.21150068901561914\n0.21078249155403597\n0.2100689545020354\n0.20936004022182975\n0.2086557113118492\n0.2079559306091236\n0.20726066119150133\n0.20656986637971061\n0.2058835097392669\n0.20520155508223228\n0.20452396646883103\n0.2038507082089263\n0.2031817448633613\n0.20251704124517156\n0.20185656242066974\n0.20120027371040963\n0.2005481406900312\n0.19990012919099373\n0.19925620530119617\n0.19861633536549367\n0.1979804859861104\n0.19734862402295342\n0.1967207165938313\n0.1960967310745805\n0.1954766350991029\n0.19486039655931767\n0.19424798360503082\n0.19363936464372536\n0.193034508340275\n0.192433383616584\n0.1918359596511567\n0.19124220587859903\n0.19065209198905445\n0.19006558792757727\n0.18948266389344584\n0.18890329033941805\n0.18832743797093093\n0.1877550777452477\n0.1871861808705543\n0.1866207188050061\n0.18605866325572948\n0.18549998617777794\n0.18494465977304617\n0.18439265648914382\n0.18384394901823026\n0.18329851029581265\n0.18275631349950916\n0.1822173320477787\n0.18168153959861932\n0.18114891004823644\n0.1806194175296824\n0.18009303641146965\n0.1795697412961583\n0.1790495070189195\n0.1785323086460763\n0.17801812147362397\n0.17750692102572882\n0.17699868305321032\n0.17649338353200392\n0.17599099866160883\n0.17549150486352028\n0.17499487877964787\n0.1745010972707203\n0.17401013741467947\n0.17352197650506235\n0.17303659204937433\n0.17255396176745172\n0.1720740635898182\n0.1715968756560323\n0.17112237631303\n0.17065054411346028\n0.1701813578140174\n0.16971479637376785\n0.16925083895247503\n0.16878946490892074\n0.1683306537992251\n0.16787438537516497\n0.16742063958249231\n0.16696939655925147\n0.1665206366340988\n0.16607434032462143\n0.1656304883356593\n0.16518906155762875\n0.16475004106484892\n0.16431340811387132\n0.16387914414181332\n0.16344723076469544\n0.16301764977578348\n0.1625903831439353\n0.16216541301195334\n0.1617427216949425\n0.16132229167867415\n0.1609041056179572\n0.16048814633501457\n0.16007439681786834\n0.15966284021873112\n0.1592534598524053\n0.15884623919469104\n0.1584411618808011\n0.158038211703785\n0.15763737261296204\n0.15723862871236227\n0.15684196425917774\n0.15644736366222164\n0.1560548114803988\n0.1556642924211839\n0.15527579133911076\n0.15488929323427164\n0.15450478325082576\n0.1541222466755191\n0.15374166893621433\n0.15336303560043069\n0.1529863323738957\n0.15261154509910643\n0.15223865975390244\n0.15186766245004926\n0.1514985394318333\n0.1511312770746674\n0.15076586188370755\n0.150402280492482\n0.1500405196615297\n0.14968056627705173\n0.1493224073495735\n0.14896603001261852\n0.14861142152139306\n0.14825856925148342\n0.14790746069756383\n0.14755808347211608\n0.14721042530416087\n0.14686447403800054\n0.14652021763197337\n0.1461776441572196\n0.14583674179645895\n0.14549749884277938\n0.14515990369843765\n0.14482394487367153\n0.14448961098552318\n0.14415689075667426\n0.14382577301429195\n0.14349624668888744\n0.14316830081318452\n0.14284192452100047\n0.14251710704613782\n0.14219383772128744\n0.14187210597694266\n0.1415519013403251\n0.14123321343432121\n0.14091603197642952\n0.1406003467777199\n0.14028614774180267\n0.13997342486380973\n0.13966216822938513\n0.13935236801368808\n0.13904401448040535\n0.13873709798077485\n0.13843160895261986\n0.13812753791939344\n0.13782487548923406\n0.13752361235403057\n0.13722373928849801\n0.1369252471492641\n0.13662812687396508\n0.13633236948035207\n0.1360379660654075\n0.13574490780447135\n0.13545318595037717\n0.13516279183259833\n0.13487371685640318\n0.13458595250202102\n0.1342994903238162\n0.13401432194947288\n0.13373043907918897\n0.13344783348487846\n0.13316649700938427\n0.13288642156569969\n0.13260759913619838\n0.13233002177187442\n0.13205368159159037\n0.1317785707813348\n0.13150468159348785\n0.1312320063460963\n0.13096053742215652\n0.1306902672689064\n0.13042118839712522\n0.1301532933804424\n0.12988657485465407\n0.12962102551704802\n0.12935663812573667\n0.12909340549899806\n0.12883132051462487\n0.12857037610928093\n0.1283105652778659\n0.12805188107288765\n0.12779431660384158\n0.12753786503659856\n0.1272825195927989\n0.12702827354925542\n0.12677512023736193\n0.12652305304251038\n0.12627206540351446\n0.1260221508120402\n0.12577330281204385\n0.12552551499921644\n0.12527878102043502\n0.12503309457322134\n0.12478844940520573\n0.12454483931359958\n0.12430225814467213\n0.12406069979323578\n0.12382015820213596\n0.12358062736174855\n0.12334210130948291\n0.1231045741292911\n0.12286803995118334\n0.12263249295074918\n0.12239792734868507\n0.1221643374103271\n0.12193171744519021\n0.12170006180651255\n0.12146936489080588\n0.12123962113741131\n0.12101082502806057\n0.12078297108644281\n0.1205560538777768\n0.12033006800838808\n0.12010500812529197\n0.11988086891578119\n0.11965764510701875\n0.11943533146563613\n0.11921392279733599\n0.11899341394650037\n0.11877379979580312\n0.11855507526582765\n0.11833723531468911\n0.11812027493766133\n0.1179041891668084\n0.11768897307062083\n0.11747462175365608\n0.11726113035618366\n0.11704849405383456\n0.11683670805725523\n0.116625767611765\n0.1164156679970192\n0.11620640452667479\n0.11599797254806127\n0.11579036744185531\n0.11558358462175916\n0.11537761953418312\n0.11517246765793211\n0.11496812450389593\n0.11476458561474309\n0.114561846564619\n0.114359902958847\n0.1141587504336339\n0.11395838465577854\n0.113758801322384\n0.11355999616057351\n0.1133619649272098\n0.11316470340861741\n0.11296820742030904\n0.11277247280671471\n0.11257749544091454\n0.11238327122437432\n0.11218979608668476\n0.11199706598530366\n0.11180507690530112\n0.11161382485910785\n0.1114233058862666\n0.11123351605318642\n0.11104445145290005\n0.110856108204824\n0.11066848245452156\n0.11048157037346876\n0.11029536815882288\n0.11010987203319395\n0.10992507824441868\n0.10974098306533743\n0.10955758279357337\n0.1093748737513147\n0.10919285228509894\n0.1090115147656002\n0.1088308575874188\n0.10865087716887306\n0.10847156995179401\n0.10829293240132205\n0.10811496100570636\n0.10793765227610616\n0.10776100274639483\n0.1075850089729659\n0.10740966753454124\n0.1072349750319819\n0.10706092808810055\n0.10688752334747642\n0.10671475747627228\n0.10654262716205352\n0.10637112911360924\n0.10620026006077538\n0.10603001675425977\n0.10586039596546945\n0.10569139448633953\n0.10552300912916424\n0.1053552367264298\n0.10518807413064926\n0.10502151821419888\n0.10485556586915662\n0.10469021400714242\n0.10452545955916004\n0.10436129947544089\n0.1041977307252892\n0.10403475029692943\n0.10387235519735492\n0.10371054245217837\n0.10354930910548384\n0.10338865221968042\n0.10322856887535778\n0.10306905617114252\n0.10291011122355694\n0.10275173116687858\n0.10259391315300208\n0.10243665435130146\n0.1022799519484949\n0.10212380314851034\n0.10196820517235254\n0.10181315525797209\n0.10165865066013477\n0.10150468865029337\n0.10135126651646015\n0.10119838156308086\n0.10104603111091025\n0.10089421249688835\n0.1007429230740186\n0.10059216021124678\n0.10044192129334178\n0.1002922037207768\n0.10014300490961218\n0.0999943222913798\nNow fitting Ionosphere\n0.23329897554382\n0.23242083213437773\n0.23154833086622953\n0.2306814222817678\n0.22982005735834088\n0.2289641875088094\n0.22811376458187085\n0.22726874086216564\n0.2264290690701716\n0.2255947023618982\n0.2247655943283894\n0.223941698995044\n0.2231229708207613\n0.2223093646969224\n0.22150083594621228\n0.2206973403212937\n0.21989883400333737\n0.2191052736004173\n0.21831661614577827\n0.21753281909598177\n0.21675384032893588\n0.21597963814181825\n0.21521017124889427\n0.21444539877923957\n0.21368528027437023\n0.2129297756857876\n0.2121788453724422\n0.21143245009812064\n0.21069055102876288\n0.2099531097297118\n0.20922008816290125\n0.20849144868398595\n0.20776715403941778\n0.2070471673634724\n0.20633145217522889\n0.20561997237550858\n0.2049126922437733\n0.20420957643498863\n0.20351058997645552\n0.2028156982646108\n0.2021248670618035\n0.20143806249304563\n0.2007552510427432\n0.2000763995514089\n0.19940147521235857\n0.19873044556839498\n0.19806327850847963\n0.19739994226439642\n0.196740405407408\n0.19608463684490737\n0.19543260581706662\n0.19478428189348446\n0.1941396349698342\n0.1934986352645142\n0.19286125331530216\n0.19222745997601418\n0.1915972264131717\n0.19097052410267518\n0.19034732482648808\n0.18972760066933128\n0.1891113240153894\n0.18849846754502933\n0.187889004231534\n0.18728290733784966\n0.18668015041335\n0.18608070729061738\n0.18548455208223985\n0.18489165917762912\n0.18430200323985552\n0.1837155592025043\n0.18313230226655144\n0.18255220789726123\n0.1819752518211056\n0.18140141002270516\n0.18083065874179352\n0.1802629744702049\n0.17969833394888504\n0.17913671416492663\n0.1785780923486287\n0.1780224459705813\n0.1774697527387758\n0.17691999059573876\n0.17637313771569366\n0.17582917250174743\n0.1752880735831031\n0.17474981981229945\n0.17421439026247626\n0.17368176422466702\n0.17315192120511824\n0.1726248409226349\n0.17210050330595422\n0.17157888849114553\n0.17105997681903737\n0.17054374883267195\n0.17003018527478678\n0.1695192670853235\n0.1690109753989634\n0.16850529154269095\n0.16800219703338368\n0.16750167357542967\n0.1670037030583716\n0.1665082675545781\n0.16601534931694203\n0.1655249307766051\n0.16503699454070944\n0.16455152339017606\n0.16406850027750922\n0.16358790832462725\n0.1631097308207201\n0.16263395122013205\n0.16216055314027117\n0.16168952035954434\n0.16122083681531696\n0.16075448660189984\n0.1602904539685599\n0.1598287233175561\n0.15936927920220118\n0.15891210632494718\n0.15845718953549617\n0.15800451382893443\n0.15755406434389208\n0.157105826360725\n0.1566597852997222\n0.15621592671933526\n0.15577423631443205\n0.15533469991457316\n0.15489730348231104\n0.15446203311151216\n0.15402887502570145\n0.15359781557642918\n0.15316884124165936\n0.15274193862418034\n0.15231709445003686\n0.15189429556698336\n0.15147352894295885\n0.1510547816645816\n0.15063804093566585\n0.15022329407575757\n0.14981052851869192\n0.1493997318111691\n0.1489908916113513\n0.14858399568747843\n0.1481790319165029\n0.14777598828274446\n0.1473748528765632\n0.1469756138930507\n0.14657825963074098\n0.14618277849033784\n0.14578915897346198\n0.1453973896814141\n0.14500745931395695\n0.14461935666811354\n0.1442330706369832\n0.14384859020857432\n0.1434659044646528\n0.14308500257960863\n0.14270587381933708\n0.14232850754013687\n0.14195289318762344\n0.14157902029565816\n0.14120687848529304\n0.14083645746372994\n0.14046774702329548\n0.1401007370404303\n0.1397354174746928\n0.13937177836777717\n0.1390098098425461\n0.1386495021020767\n0.1382908454287206\n0.13793383018317756\n0.13757844680358225\n0.13722468580460445\n0.13687253777656203\n0.1365219933845471\n0.13617304336756395\n0.13582567853768052\n0.1354798897791913\n0.13513566804779256\n0.13479300436977\n0.1344518898411966\n0.13411231562714418\n0.1337742729609047\n0.13343775314322334\n0.13310274754154247\n0.132769247589257\n0.13243724478497998\n0.13210673069181852\n0.13177769693666108\n0.13145013520947396\n0.13112403726260843\n0.1307993949101179\n0.13047620002708443\n0.13015444454895533\n0.12983412047088913\n0.12951521984711073\n0.12919773479027577\n0.12888165747084437\n0.12856698011646378\n0.1282536950113591\n0.1279417944957339\n0.12763127096517804\n0.1273221168700849\n0.12701432471507645\n0.12670788705843639\n0.12640279651155137\n0.1260990457383602\n0.1257966274548108\n0.12549553442832462\n0.12519575947726863\n0.12489729547043482\n0.12460013532652706\n0.12430427201365479\n0.12400969854883398\n0.12371640799749532\n0.12342439347299848\n0.12313364813615409\n0.12284416519475119\n0.12255593790309258\n0.12226895956153525\n0.1219832235160378\n0.12169872315771443\n0.12141545192239382\n0.12113340329018552\n0.12085257078505114\n0.12057294797438195\n0.12029452846858169\n0.12001730592065593\n0.11974127402580595\n0.11946642652102878\n0.11919275718472223\n0.11892025983629552\n0.1186489283357849\n0.1183787565834745\n0.11810973851952204\n0.11784186812359\n0.11757513941448092\n0.1173095464497781\n0.11704508332549084\n0.11678174417570437\n0.11651952317223421\n0.11625841452428529\n0.11599841247811511\n0.1157395113167019\n0.11548170535941636\n0.11522498896169808\n0.11496935651473603\n0.11471480244515309\n0.1144613212146947\n0.11420890731992105\n0.11395755529190416\n0.11370725969592757\n0.11345801513119062\n0.11320981623051646\n0.112962657660063\n0.11271653411903848\n0.11247144033941965\n0.11222737108567406\n0.11198432115448552\n0.11174228537448291\n0.1115012586059726\n0.11126123574067363\n0.1110222117014568\n0.1107841814420862\n0.11054713994696441\n0.1103110822308806\n0.11007600333876157\n0.10984189834542582\n0.10960876235534048\n0.10937659050238159\n0.10914537794959638\n0.10891511988896903\n0.10868581154118906\n0.10845744815542208\n0.10823002500908373\n0.1080035374076157\n0.10777798068426508\n0.10755335019986516\n0.10732964134261998\n0.10710684952789039\n0.106884970197983\n0.10666399882194141\n0.10644393089534\n0.10622476194007935\n0.10600648750418518\n0.10578910316160821\n0.10557260451202712\n0.10535698718065332\n0.10514224681803826\n0.10492837909988205\n0.10471537972684525\n0.10450324442436165\n0.10429196894245411\n0.10408154905555143\n0.10387198056230784\n0.10366325928542425\n0.10345538107147127\n0.10324834179071421\n0.10304213733694018\n0.10283676362728637\n0.10263221660207084\n0.10242849222462477\n0.10222558648112628\n0.10202349538043645\n0.10182221495393629\n0.10162174125536641\n0.10142207036066729\n0.10122319836782183\n0.10102512139669931\n0.10082783558890115\n0.10063133710760755\n0.10043562213742648\n0.10024068688424383\n0.10004652757507487\n0.09985314045791746\nNow fitting Ionosphere\n0.21861329547670166\n0.21776924328443267\n0.21693163198391519\n0.21610039811435092\n0.2152754787133599\n0.214456811322066\n0.21364433398971833\n0.21283798527786735\n0.2120377042641147\n0.21124343054544967\n0.21045510424119326\n0.20967266599556206\n0.20889605697986985\n0.20812521889438043\n0.20736009396982757\n0.20660062496861362\n0.20584675518570442\n0.20509842844922976\n0.20435558912080506\n0.20361818209558458\n0.2028861528020597\n0.20215944720161355\n0.20143801178784201\n0.20072179358565384\n0.2000107401501593\n0.19930479956535654\n0.19860392044262817\n0.19790805191905475\n0.19721714365555557\n0.19653114583486614\n0.19585000915935885\n0.19517368484871744\n0.19450212463747119\n0.193835280772398\n0.19317310600980211\n0.19251555361267556\n0.19186257734774703\n0.19121413148242908\n0.19057017078166558\n0.1899306505046886\n0.18929552640168862\n0.18866475471040509\n0.18803829215264198\n0.18741609593071334\n0.1867981237238243\n0.1861843336843926\n0.18557468443431413\n0.18496913506117785\n0.1843676451144342\n0.18377017460152054\n0.18317668398394818\n0.18258713417335456\n0.18200148652752324\n0.18141970284637668\n0.18084174536794448\n0.18026757676430907\n0.17969716013753356\n0.17913045901557417\n0.1785674373481788\n0.17800805950277665\n0.17745229026035822\n0.17690009481135188\n0.17635143875149584\n0.17580628807771004\n0.17526460918396855\n0.17472636885717624\n0.17419153427304868\n0.17366007299200065\n0.17313195295504238\n0.17260714247968528\n0.172085610255861\n0.1715673253418523\n0.17105225716023872\n0.1705403754938586\n0.17003165048178803\n0.1695260526153377\n0.16902355273406966\n0.1685241220218344\n0.16802773200282936\n0.16753435453767973\n0.16704396181954298\n0.16655652637023716\n0.1660720210363936\n0.16559041898563664\n0.16511169370278825\n0.16463581898610066\n0.16416276894351542\n0.16369251798895182\n0.16322504083862274\n0.16276031250738082\n0.16229830830509295\n0.16183900383304545\n0.16138237498037916\n0.16092839792055572\n0.16047704910785404\n0.1600283052738985\n0.15958214342421873\n0.15913854083484025\n0.15869747504890835\n0.15825892387334276\n0.15782286537552523\n0.15738927788001908\n0.1569581399653218\n0.15652943046064904\n0.15610312844275248\n0.15567921323276962\n0.1552576643931061\n0.15483846172435065\n0.15442158526222347\n0.1540070152745566\n0.15359473225830644\n0.15318471693659982\n0.1527769502558118\n0.15237141338267624\n0.15196808770142836\n0.1515669548109798\n0.1511679965221252\n0.1507711948547816\n0.15037653203525891\n0.14998399049356256\n0.1495935528607271\n0.14920520196618178\n0.14881892083514683\n0.1484346926860612\n0.14805250092804073\n0.14767232915836706\n0.14729416116000743\n0.1469179808991637\n0.1465437725228529\n0.14617152035651587\n0.1458012089016572\n0.14543282283351341\n0.14506634699875037\n0.14470176641319044\n0.14433906625956763\n0.14397823188531098\n0.14361924880035745\n0.1432621026749909\n0.1429067793377106\n0.14255326477312608\n0.14220154511987973\n0.14185160666859623\n0.14150343585985867\n0.14115701928221147\n0.14081234367018902\n0.14046939590237076\n0.14012816299946163\n0.13978863212239837\n0.13945079057048054\n0.13911462577952707\n0.13878012532005696\n0.13844727689549427\n0.13811606834039844\n0.137786487618717\n0.13745852282206286\n0.13713216216801494\n0.13680739399844133\n0.13648420677784626\n0.1361625890917389\n0.1358425296450246\n0.13552401726041888\n0.13520704087688265\n0.13489158954807914\n0.13457765244085232\n0.1342652188337265\n0.1339542781154261\n0.13364481978341747\n0.13333683344246933\n0.13303030880323444\n0.13272523568085076\n0.13242160399356245\n0.13211940376135986\n0.1318186251046392\n0.13151925824288024\n0.131221293493344\n0.13092472126978716\n0.13062953208119668\n0.13033571653054032\n0.1300432653135365\n0.1297521692174408\n0.1294624191198498\n0.12917400598752277\n0.1288869208752189\n0.12860115492455226\n0.12831669936286247\n0.128033545502102\n0.12775168473773907\n0.12747110854767674\n0.1271918084911868\n0.12691377620785999\n0.12663700341657053\n0.12636148191445581\n0.12608720357591113\n0.12581416035159843\n0.1255423442674698\n0.12527174742380462\n0.1250023619942615\n0.12473418022494266\n0.12446719443347312\n0.12420139700809261\n0.12393678040676065\n0.12367333715627499\n0.12341105985140209\n0.12314994115402145\n0.12288997379228116\n0.12263115055976656\n0.12237346431468059\n0.12211690797903615\n0.12186147453786027\n0.12160715703841012\n0.12135394858939971\n0.12110184236023872\n0.12085083158028198\n0.12060090953808976\n0.12035206958069929\n0.12010430511290654\n0.11985760959655856\n0.1196119765498562\n0.1193673995466671\n0.11912387221584839\n0.11888138824057955\n0.11863994135770496\n0.11839952535708567\n0.1181601340809615\n0.11792176142332128\n0.11768440132928333\n0.11744804779448417\n0.1172126948644765\n0.11697833663413618\n0.11674496724707728\n0.11651258089507611\n0.11628117181750333\n0.11605073430076444\n0.11582126267774862\n0.11559275132728518\n0.11536519467360795\n0.11513858718582809\n0.11491292337741338\n0.11468819780567623\n0.11446440507126837\n0.11424153981768313\n0.1140195967307648\n0.11379857053822523\n0.11357845600916747\n0.11335924795361592\n0.11314094122205355\n0.11292353070496591\n0.1127070113323913\n0.11249137807347775\n0.1122766259360465\n0.11206274996616089\n0.11184974524770279\n0.11163760690195375\n0.11142633008718328\n0.11121590999824209\n0.11100634186616179\n0.11079762095776001\n0.11058974257525121\n0.11038270205586298\n0.11017649477145788\n0.10997111612816059\n0.10976656156599045\n0.10956282655849901\n0.10935990661241286\n0.10915779726728153\n0.10895649409513028\n0.10875599270011767\n0.10855628871819825\n0.10835737781678972\n0.1081592556944448\n0.10796191808052784\n0.1077653607348958\n0.10756957944758372\n0.10737457003849475\n0.10718032835709397\n0.10698685028210725\n0.10679413172122353\n0.10660216861080175\n0.10641095691558147\n0.10622049262839771\n0.10603077176989983\n0.10584179038827352\n0.10565354455896796\n0.10546603038442515\n0.1052792439938142\n0.10509318154276857\n0.10490783921312709\n0.10472321321267843\n0.10453929977490906\n0.10435609515875474\n0.10417359564835509\n0.10399179755281165\n0.10381069720594932\n0.10363029096608045\n0.10345057521577297\n0.10327154636162075\n0.10309320083401752\n0.10291553508693348\n0.10273854559769523\n0.10256222886676843\n0.10238658141754299\n0.1022115997961218\n0.10203728057111162\n0.10186362033341682\n0.10169061569603613\n0.10151826329386165\n0.10134655978348073\n0.10117550184298015\n0.10100508617175313\n0.10083530949030876\n0.10066616854008352\n0.10049766008325581\n0.10032978090256213\n0.10016252780111659\n0.09999589760223142\nLR accuracy score: 0.7178571428571429\nLR precision score: 0.6980203193033382\nLR recall score: 0.9888803088803089\nLR f1 score: 0.8171551691235773\n"
    }
   ],
   "source": [
    "\n",
    "# %% Using evaluate_acc to do 5-fold CV\n",
    "\n",
    "# LOGISTIC REGRESSION MODEL \n",
    "# APPEND SCORES\n",
    "vali_acc_score_lr=[]; vali_preci_socre_lr=[]; vali_recall_score_lr=[];vali_f1_score_lr=[]\n",
    "# implement LR model \n",
    "import Models.logisticRegression as lr\n",
    "import Models.evaluate_acc as acc\n",
    "# LogReg1 =LogisticRegression()\n",
    "\n",
    "for i in range(5):\n",
    "    validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,i+1)\n",
    "    training_data_ion=training_data_ion.astype('int'); training_labels_ion=training_labels_ion.astype('int')\n",
    "    validate_data_ion=validate_data_ion.astype('int'); validate_labels_ion=validate_labels_ion.astype('int')\n",
    "    \n",
    "    # delete the previous loop's model \n",
    "    # if i > 0:\n",
    "    #    del LogReg1\n",
    " \n",
    "    ionslr1 = lr.Logistic_Regression(0.02,\"Ionosphere\",\"binary\")                # build model\n",
    "    params1 = ionslr1.fit(training_data_ion,training_labels_ion,0.02,1e-1)      # find parameters to predict \n",
    "    validate_pred_lr= ionslr1.predict(params1,validate_data_ion)                # predict on validation data \n",
    "    validate_pred_lr=validate_pred_lr * 1                                       # change True of False from validate_pred_lr into binary \n",
    "    \n",
    "    # accuracy score \n",
    "    validate_expect_lr = validate_labels_ion\n",
    "    tp, tn, fn, fp = acc.compute_tp_tn_fn_fp(validate_expect_lr,validate_pred_lr) # for other score computation \n",
    "    conf_mat = acc.compute_tp_tn_fn_fp(validate_expect_lr,validate_pred_lr)     # confusion matrix: (tp, tn, fn, fp)\n",
    "    validate_acc_score_lr = (acc.compute_accuracy(*list(conf_mat)))*0.01        # compute accuracy score   \n",
    "    vali_acc_score_lr.append(validate_acc_score_lr)                             # append validation accuracy score for each experiment \n",
    "    \n",
    "    # precision score \n",
    "    validate_preci_score_lr = acc.compute_precision(tp, fp)*0.01\n",
    "    vali_preci_socre_lr.append(validate_preci_score_lr)    \n",
    "\n",
    "    # recall score \n",
    "    validate_recall_score_lr = acc.compute_recall(tp, fn)*0.01\n",
    "    vali_recall_score_lr.append(validate_recall_score_lr)\n",
    "    \n",
    "    # f1 score \n",
    "    validate_f1_score_lr = acc.compute_f1_score(validate_expect_lr,validate_pred_lr)\n",
    "    vali_f1_score_lr.append(validate_f1_score_lr)\n",
    "    \n",
    "\n",
    "\n",
    "mu_vali_acc_lr=np.mean(vali_acc_score_lr); \n",
    "mu_vali_preci_lr=np.mean(vali_preci_socre_lr); \n",
    "mu_vali_recall_lr=np.mean(vali_recall_score_lr);\n",
    "mu_vali_f1_lr=np.mean(vali_f1_score_lr)\n",
    "print('LR accuracy score:',mu_vali_acc_lr)\n",
    "print('LR precision score:',mu_vali_preci_lr)\n",
    "print('LR recall score:',mu_vali_recall_lr)\n",
    "print('LR f1 score:',mu_vali_f1_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Now fitting ionosphere\n[1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0]\n(224, 2)\n(224, 34)\n(224, 34)\n(224, 2)\n(2, 34)\n(2, 34)\nlog prior\n[[-1.0171969 ]\n [-0.44880142]]\nlikelihood \n[[-2.55647005e+00 -7.08733425e-03 -1.63219021e+01 -1.48095565e+01\n  -2.48239598e+00 -1.32848651e+01 -1.01943301e+00 -7.10585277e+00\n  -1.14330133e+01 -9.82395976e-01 -3.44535894e+00 -5.58116141e+00\n  -7.08733425e-03 -1.23589392e+01 -7.08733425e-03 -7.08733425e-03\n  -7.08733425e-03 -7.08733425e-03 -7.08733425e-03 -7.08733425e-03\n  -1.55647005e+00 -5.19433013e-01 -9.94741655e-01 -2.95770462e+00\n  -7.08733425e-03 -1.50708733e+00 -7.08733425e-03 -5.07087334e-01\n  -7.08733425e-03 -1.55647005e+00 -4.49474166e+00 -7.08733425e-03\n  -7.08733425e-03 -7.08733425e-03 -3.43301326e+00 -7.08733425e-03\n  -2.45770462e+00 -2.94535894e+00 -7.08733425e-03 -7.08733425e-03\n  -5.07087334e-01 -1.14083219e+01 -7.08733425e-03 -4.82395976e-01\n  -1.00708733e+00 -4.45358939e-01 -2.45770462e+00 -2.47005030e+00\n  -5.07087334e-01 -3.04412437e+00 -1.47005030e+00 -7.78486511e+00\n  -7.08733425e-03 -7.08733425e-03 -7.08733425e-03 -7.08733425e-03]\n [-2.48503594e+00 -6.01496406e-03 -1.60724485e+01 -1.46353856e+01\n  -2.44307790e+00 -1.30864345e+01 -1.02699399e+00 -7.08993105e+00\n  -1.14360849e+01 -9.43077901e-01 -3.28923175e+00 -5.47804294e+00\n  -6.01496406e-03 -1.25619590e+01 -6.01496406e-03 -6.01496406e-03\n  -6.01496406e-03 -6.01496406e-03 -6.01496406e-03 -6.01496406e-03\n  -1.52000098e+00 -4.71049929e-01 -9.71049929e-01 -2.99202895e+00\n  -6.01496406e-03 -1.38713385e+00 -6.01496406e-03 -4.64056922e-01\n  -6.01496406e-03 -1.54797301e+00 -4.35216881e+00 -6.01496406e-03\n  -6.01496406e-03 -6.01496406e-03 -3.34517580e+00 -6.01496406e-03\n  -2.33818280e+00 -2.81021077e+00 -6.01496406e-03 -6.01496406e-03\n  -5.13007971e-01 -1.14850359e+01 -6.01496406e-03 -4.85035943e-01\n  -9.71049929e-01 -4.85035943e-01 -2.38014084e+00 -2.33118979e+00\n  -5.26993985e-01 -3.18084014e+00 -1.39412685e+00 -7.64937161e+00\n  -6.01496406e-03 -6.01496406e-03 -6.01496406e-03 -6.01496406e-03]]\n(2, 56)\n(224,)\n(224,)\n(34, 224)\n"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (224,) (2,56) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b60cfb0a8e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mionnb\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaive_Bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ionosphere\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mionnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_ion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels_ion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_data_ion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mionnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest_ion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/COMP551A1/Models/naive_bayes_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, training_data, training_labels, test_data)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbinary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (224,) (2,56) "
     ]
    }
   ],
   "source": [
    "import Models.naive_bayes_v2 as nb \n",
    "import numpy as np \n",
    "feature_types= np.array([\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",\"continuous\",])\n",
    "\n",
    "ionnb= nb.Naive_Bayes(\"ionosphere\", \"binary\", feature_types)\n",
    "parameters= ionnb.fit(training_data_ion, training_labels_ion, validate_data_ion)\n",
    "results= ionnb.predict(xTest_ion, parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}