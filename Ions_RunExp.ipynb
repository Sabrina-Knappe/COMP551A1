{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bit16c59f3069ba44c59fdc0e453d119bc5",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1  0  0.99539  -0.05889  0.85243  0.02306  0.83398  -0.37708      1.1  \\\n0    1  0  1.00000  -0.18829  0.93035 -0.36156 -0.10868  -0.93597  1.00000   \n1    1  0  1.00000  -0.03365  1.00000  0.00485  1.00000  -0.12062  0.88965   \n2    1  0  1.00000  -0.45161  1.00000  1.00000  0.71216  -1.00000  0.00000   \n3    1  0  1.00000  -0.02401  0.94140  0.06531  0.92106  -0.23255  0.77152   \n4    1  0  0.02337  -0.00592 -0.09924 -0.11949 -0.00763  -0.11824  0.14706   \n5    1  0  0.97588  -0.10602  0.94601 -0.20800  0.92806  -0.28350  0.85996   \n6    0  0  0.00000   0.00000  0.00000  0.00000  1.00000  -1.00000  0.00000   \n7    1  0  0.96355  -0.07198  1.00000 -0.14333  1.00000  -0.21313  1.00000   \n8    1  0 -0.01864  -0.08459  0.00000  0.00000  0.00000   0.00000  0.11470   \n9    1  0  1.00000   0.06655  1.00000 -0.18388  1.00000  -0.27320  1.00000   \n10   1  0  1.00000  -0.54210  1.00000 -1.00000  1.00000  -1.00000  1.00000   \n11   1  0  1.00000  -0.16316  1.00000 -0.10169  0.99999  -0.15197  1.00000   \n12   1  0  1.00000  -0.86701  1.00000  0.22280  0.85492  -0.39896  1.00000   \n13   1  0  1.00000   0.07380  1.00000  0.03420  1.00000  -0.05563  1.00000   \n14   1  0  0.50932  -0.93996  1.00000  0.26708 -0.03520  -1.00000  1.00000   \n15   1  0  0.99645   0.06468  1.00000 -0.01236  0.97811   0.02498  0.96112   \n16   0  0  0.00000   0.00000 -1.00000 -1.00000  1.00000   1.00000 -1.00000   \n17   1  0  0.67065   0.02528  0.66626  0.05031  0.57197   0.18761  0.08776   \n18   0  0  1.00000  -1.00000  0.00000  0.00000  0.00000   0.00000  1.00000   \n19   1  0  1.00000  -0.00612  1.00000 -0.09834  1.00000  -0.07649  1.00000   \n20   0  0  1.00000   1.00000  0.00000  0.00000  0.00000   0.00000 -1.00000   \n21   1  0  0.96071   0.07088  1.00000  0.04296  1.00000   0.09313  0.90169   \n22   0  0 -1.00000   1.00000  0.00000  0.00000  0.00000   0.00000 -1.00000   \n23   1  0  1.00000  -0.06182  1.00000  0.02942  1.00000  -0.05131  1.00000   \n24   1  0  1.00000   0.57820  1.00000 -1.00000  1.00000  -1.00000  1.00000   \n25   1  0  1.00000  -0.08714  1.00000 -0.17263  0.86635  -0.81779  0.94817   \n26   0  0 -1.00000  -1.00000  0.00000  0.00000 -1.00000   1.00000  1.00000   \n27   1  0  1.00000   0.08380  1.00000  0.17387  1.00000  -0.13308  0.98172   \n28   0  0 -1.00000  -1.00000  1.00000  1.00000  1.00000  -1.00000 -1.00000   \n29   1  0  1.00000  -0.14236  1.00000 -0.16256  1.00000  -0.23656  1.00000   \n..  .. ..      ...       ...      ...      ...      ...       ...      ...   \n320  1  0  0.89505  -0.03168  0.87525  0.05545  0.89505   0.01386  0.92871   \n321  1  0  0.90071   0.01773  1.00000 -0.01773  0.90071   0.00709  0.84752   \n322  1  0  0.39394  -0.24242  0.62655  0.01270  0.45455   0.09091  0.63636   \n323  1  0  0.86689   0.35950  0.72014  0.66667  0.37201   0.83049  0.08646   \n324  1  0  0.89563   0.37917  0.67311  0.69438  0.35916   0.88696 -0.04193   \n325  1  0  0.90547   0.41113  0.65354  0.74761  0.29921   0.95905 -0.13342   \n326  1  0  1.00000   1.00000  0.36700  0.06158  0.12993   0.92713 -0.27586   \n327  1  0  1.00000   0.51515  0.45455  0.33333  0.06061   0.36364 -0.32104   \n328  1  0  0.88110   0.00000  0.94817 -0.02744  0.93598  -0.01220  0.90244   \n329  1  0  0.82624   0.08156  0.79078 -0.08156  0.90426  -0.01773  0.92908   \n330  1  0  0.74468   0.10638  0.88706  0.00982  0.88542   0.01471  0.87234   \n331  1  0  0.87578   0.03727  0.89951  0.00343  0.89210   0.00510  0.86335   \n332  1  0  0.97513   0.00710  0.98579  0.01954  1.00000   0.01954  0.99290   \n333  1  0  1.00000   0.01105  1.00000  0.01105  1.00000   0.02320  0.99448   \n334  1  0  1.00000  -0.01342  1.00000  0.01566  1.00000  -0.00224  1.00000   \n335  1  0  0.88420   0.36724  0.67123  0.67382  0.39613   0.86399  0.02424   \n336  1  0  0.90147   0.41786  0.64131  0.75725  0.30440   0.95148 -0.20449   \n337  1  0  0.32789   0.11042  0.15970  0.29308  0.14020   0.74485 -0.25131   \n338  1  0  0.65845   0.43617  0.44681  0.74804  0.05319   0.85106 -0.32027   \n339  1  0  0.19466   0.05725  0.04198  0.25191 -0.10557   0.48866 -0.18321   \n340  1  0  0.98002   0.00075  1.00000  0.00000  0.98982  -0.00075  0.94721   \n341  1  0  0.82254  -0.07572  0.80462  0.00231  0.87514  -0.01214  0.86821   \n342  1  0  0.35346  -0.13768  0.69387 -0.02423  0.68195  -0.03574  0.55717   \n343  1  0  0.76046   0.01092  0.86335  0.00258  0.85821   0.00384  0.79988   \n344  1  0  0.66667  -0.01366  0.97404  0.06831  0.49590   0.50137  0.75683   \n345  1  0  0.83508   0.08298  0.73739 -0.14706  0.84349  -0.05567  0.90441   \n346  1  0  0.95113   0.00419  0.95183 -0.02723  0.93438  -0.01920  0.94590   \n347  1  0  0.94701  -0.00034  0.93207 -0.03227  0.95177  -0.03431  0.95584   \n348  1  0  0.90608  -0.01657  0.98122 -0.01989  0.95691  -0.03646  0.85746   \n349  1  0  0.84710   0.13533  0.73638 -0.06151  0.87873   0.08260  0.88928   \n\n     0.03760  ...  -0.51171  0.41078  -0.46168  0.21266  -0.34090  0.42267  \\\n0   -0.04549  ...  -0.26569 -0.20468  -0.18401 -0.19040  -0.11593 -0.16626   \n1    0.01198  ...  -0.40220  0.58984  -0.22145  0.43100  -0.17365  0.60436   \n2    0.00000  ...   0.90695  0.51613   1.00000  1.00000  -0.20099  0.25682   \n3   -0.16399  ...  -0.65158  0.13290  -0.53206  0.02431  -0.62197 -0.05707   \n4    0.06637  ...  -0.01535 -0.03240   0.09223 -0.07859   0.00732  0.00000   \n5   -0.27342  ...  -0.81634  0.13659  -0.82510  0.04606  -0.82395 -0.04262   \n6    0.00000  ...   1.00000  1.00000   1.00000  0.00000   0.00000  1.00000   \n7   -0.36174  ...  -0.65440  0.57577  -0.69712  0.25435  -0.63919  0.45114   \n8   -0.26810  ...  -0.01326  0.20645  -0.02294  0.00000   0.00000  0.16595   \n9   -0.43107  ...  -0.89128  0.47211  -0.86500  0.40303  -0.83675  0.30996   \n10   0.36217  ...  -0.40888  1.00000  -0.62745  1.00000  -1.00000  1.00000   \n11  -0.19277  ...  -0.47137  0.76224  -0.58370  0.65723  -0.68794  0.68714   \n12  -0.12090  ...  -0.17012  1.00000   0.35924  1.00000  -0.66494  1.00000   \n13   0.08764  ...   0.20033  1.00000   0.36743  0.95603   0.48641  1.00000   \n14  -1.00000  ...   0.92236  0.39752   0.26501  0.00000   0.00000  1.00000   \n15   0.02312  ...   0.13412  0.79476   0.13638  0.79110   0.15379  0.77122   \n16   1.00000  ...   1.00000  1.00000  -1.00000 -1.00000   1.00000 -1.00000   \n17   0.34081  ...   0.23724  0.46167   0.24618  0.43433   0.25306  0.40663   \n18   1.00000  ...   1.00000  1.00000   1.00000  1.00000  -1.00000  1.00000   \n19  -0.10605  ...  -0.40372  0.82681  -0.42231  0.75784  -0.38231  0.80448   \n20  -1.00000  ...   0.00000  1.00000  -1.00000 -1.00000   1.00000 -1.00000   \n21  -0.05144  ...  -0.07705  0.58051  -0.02205  0.49664  -0.01251  0.51310   \n22   1.00000  ...  -1.00000  1.00000  -1.00000  1.00000   1.00000 -1.00000   \n23  -0.01707  ...  -0.34838  0.72529  -0.29174  0.73094  -0.38576  0.54356   \n24  -1.00000  ...  -1.00000  1.00000  -1.00000  1.00000  -1.00000  1.00000   \n25   0.61053  ...  -0.82868  0.48136  -0.86583  0.40650  -0.89674  0.32984   \n26  -0.37500  ...  -1.00000 -1.00000   1.00000 -1.00000  -1.00000  0.00000   \n27   0.64520  ...   1.00000  0.83899   1.00000  0.74822   1.00000  0.64358   \n28   1.00000  ...   1.00000  1.00000  -1.00000  1.00000  -1.00000 -1.00000   \n29  -0.07514  ...  -0.47643  0.98820  -0.49687  1.00000  -0.75820  1.00000   \n..       ...  ...       ...      ...       ...      ...       ...      ...   \n320  0.02772  ...   0.02376  0.89002   0.01611  0.88119   0.00198  0.87327   \n321  0.05674  ...   0.04610  0.94305   0.03247  0.94681   0.02482  1.00000   \n322  0.09091  ...   0.05929  0.46362   0.06142  0.33333  -0.03030  0.41856   \n323  0.85893  ...  -0.59954  0.15360  -0.53127  0.32309  -0.37088  0.46189   \n324  0.93345  ...  -0.73856  0.33531  -0.62296  0.52414  -0.42086  0.61217   \n325  0.97820  ...  -0.85368  0.67538  -0.61959  0.85977  -0.28123  0.88654   \n326  0.93596  ...  -0.45950  0.85471  -0.06831  1.00000   1.00000  0.38670   \n327  0.73062  ...  -0.39394  0.72961   0.12331  0.96970   0.57576  0.24242   \n328  0.01829  ...   0.04878  0.89666   0.02226  0.90854   0.00915  1.00000   \n329  0.01064  ...   0.15957  0.89527   0.08165  0.77660   0.06738  0.92553   \n330 -0.01418  ...   0.11348  0.83429   0.06014  0.74468  -0.03546  0.81710   \n331  0.00000  ...  -0.04348  0.82111   0.02033  0.81988   0.08696  0.80757   \n332  0.01599  ...   0.03552  0.97540   0.06477  0.94849   0.08171  0.99112   \n333 -0.01436  ...  -0.00663  0.98033   0.01600  0.97901   0.01547  0.98564   \n334  0.06264  ...   0.10067  0.99989   0.08763  0.99105   0.08501  1.00000   \n335  0.93182  ...  -0.69774  0.26028  -0.60678  0.44569  -0.43383  0.54209   \n336  0.96534  ...  -0.82154  0.74105  -0.55231  0.89415  -0.18725  0.87893   \n337  0.91993  ...  -0.18560  0.39599  -0.11498  0.31005   0.05614  0.21443   \n338  0.82139  ...  -0.18645  0.74758   0.23713  0.45185   0.59071  0.20549   \n339 -0.18321  ...   0.24692  0.03913   0.31092 -0.03817   0.26336 -0.16794   \n340  0.02394  ...   0.08107  0.96709   0.07255  0.95701   0.08088  0.98190   \n341 -0.07514  ...  -0.02370  0.76717  -0.02731  0.74046  -0.07630  0.70058   \n342 -0.06119  ...  -0.00564  0.39146  -0.09038  0.35588  -0.10306  0.32232   \n343  0.02304  ...  -0.00303  0.70886   0.01375  0.66161   0.00849  0.66298   \n344 -0.00273  ...   0.06967  0.68656   0.17088  0.87568   0.07787  0.55328   \n345 -0.04622  ...  -0.04202  0.83479   0.00123  1.00000   0.12815  0.86660   \n346  0.01606  ...   0.01361  0.93522   0.04925  0.93159   0.08168  0.94066   \n347  0.02446  ...   0.03193  0.92489   0.02542  0.92120   0.02242  0.92459   \n348  0.00110  ...  -0.02099  0.89147  -0.07760  0.82983  -0.17238  0.96022   \n349 -0.09139  ...  -0.15114  0.81147  -0.04822  0.78207  -0.00703  0.75747   \n\n     -0.54487  0.18641  -0.45300  g  \n0    -0.06288 -0.13738  -0.02447  b  \n1    -0.24180  0.56045  -0.38238  g  \n2     1.00000 -0.32382   1.00000  b  \n3    -0.59573 -0.04608  -0.65697  g  \n4     0.00000 -0.00039   0.12011  b  \n5    -0.81318 -0.13832  -0.80975  g  \n6     1.00000  0.00000   0.00000  b  \n7    -0.72779  0.38895  -0.73420  g  \n8     0.24086 -0.08208   0.38065  b  \n9    -0.89093  0.22995  -0.89158  g  \n10   -1.00000  1.00000  -1.00000  b  \n11   -0.64537  0.64727  -0.67226  g  \n12    0.88428  1.00000  -0.18826  b  \n13    0.32492  1.00000   0.46712  g  \n14    0.23188  0.00000   0.00000  b  \n15    0.15930  0.70941   0.12015  g  \n16   -1.00000  1.00000  -1.00000  b  \n17    0.25792  1.00000   0.33036  g  \n18    1.00000  1.00000   1.00000  b  \n19   -0.40575  0.74354  -0.45039  g  \n20    1.00000 -1.00000   1.00000  b  \n21   -0.00015  0.52099  -0.00182  g  \n22    1.00000  0.00000   0.00000  b  \n23   -0.26284  0.64207  -0.39487  g  \n24   -1.00000  1.00000  -1.00000  b  \n25   -0.92128 -0.13341  -1.00000  g  \n26    0.00000 -1.00000   1.00000  b  \n27    1.00000  0.52479   1.00000  g  \n28    1.00000  1.00000  -1.00000  b  \n29   -0.75761  1.00000  -0.84437  g  \n..        ...      ...       ... ..  \n320   0.04158  0.86733   0.02376  g  \n321   0.01064  0.93617   0.02128  g  \n322   0.06410  0.39394   0.24242  g  \n323  -0.19681  0.40956   0.01820  g  \n324  -0.17343  0.60073   0.08660  g  \n325   0.09800  0.75495   0.46301  g  \n326   0.00246  0.17758   0.79790  g  \n327   0.36364  0.09091   0.33333  g  \n328   0.05488  0.97561  -0.01220  g  \n329   0.18085  0.92553   0.00000  g  \n330   0.06800  0.80774   0.07173  g  \n331   0.02308  0.80088   0.02441  g  \n332   0.06217  0.98934   0.09947  g  \n333   0.02099  0.98674   0.02762  g  \n334   0.10067  1.00000   0.10067  g  \n335  -0.21542  0.56286   0.02823  g  \n336   0.20359  0.70555   0.54852  g  \n337   0.20540  0.13376   0.26422  g  \n338   0.76764 -0.18533   0.74356  g  \n339   0.16794 -0.30153  -0.33588  g  \n340   0.08126  0.97247   0.08616  g  \n341  -0.04220  0.78439   0.01214  g  \n342  -0.08637  0.28943  -0.08300  g  \n343   0.01484  0.63887   0.01525  g  \n344   0.24590  0.13934   0.48087  g  \n345  -0.10714  0.90546  -0.04307  g  \n346  -0.00035  0.91483   0.04712  g  \n347   0.00442  0.92697  -0.00577  g  \n348  -0.03757  0.87403  -0.16243  g  \n349  -0.06678  0.85764  -0.06151  g  \n\n[350 rows x 35 columns]\n"
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'ionosphere-data.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('ionosphere-data.csv', sep=',')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n 'b' 'g' 'b' 'g' 'b' 'g' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g']\n"
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy\n",
    "#df.to_numpy()\n",
    "temp = df.values\n",
    "R,C = temp.shape\n",
    "#print(C)\n",
    "\n",
    "# Split array into design matrix and labels\n",
    "ionosphere_labels = temp[:, C-1]\n",
    "print(ionosphere_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1 0 1.0 ... -0.06287999999999999 -0.13738 -0.02447]\n [1 0 1.0 ... -0.2418 0.56045 -0.38238]\n [1 0 1.0 ... 1.0 -0.32382 1.0]\n ...\n [1 0 0.94701 ... 0.00442 0.9269700000000001 -0.00577]\n [1 0 0.9060799999999999 ... -0.03757 0.87403 -0.16243]\n [1 0 0.8471 ... -0.06677999999999999 0.85764 -0.06151]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Remove labels to get design matrix\n",
    "ionosphere_design_matrix= np.delete(temp, C-1, 1)\n",
    "print(ionosphere_design_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0\n 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1\n 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0\n 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1]\n[0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1]\n"
    }
   ],
   "source": [
    "import Models.kfold_CV_try as cv\n",
    "\n",
    "# Train test split\n",
    "xTrain_ion, xTest_ion, yTrain_ion, yTest_ion = cv.split_train_test(ionosphere_design_matrix, ionosphere_labels, 0.2)\n",
    "# print(xTest_ion); print(xTrain_ion); print(yTest_ion);print(yTrain_ion)\n",
    "# change the binary label 'g' and 'b' into 1 and 0\n",
    "for i in enumerate(yTrain_ion):\n",
    "    if i[1] == 'g':\n",
    "        yTrain_ion[i[0]] = 1\n",
    "    elif i[1] == 'b':\n",
    "        yTrain_ion[i[0]] = 0\n",
    "yTrain_ion=yTrain_ion.astype('int')\n",
    "for i in enumerate(yTest_ion):\n",
    "    if i[1] == 'g':\n",
    "        yTest_ion[i[0]] = 1\n",
    "    elif i[1] == 'b':\n",
    "        yTest_ion[i[0]] = 0        \n",
    "yTest_ion=yTest_ion.astype('int')\n",
    "\n",
    "print(yTrain_ion);print(yTest_ion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1 0 0.08696 ... -0.07322999999999999 0.1167 -0.06865]\n [1 0 0.058660000000000004 ... 0.0 0.04749 -0.03352]\n [1 0 0.59887 ... -0.31262 0.6742 -0.22034]\n ...\n [0 0 1.0 ... -1.0 -1.0 -1.0]\n [1 0 0.47336999999999996 ... -0.14497000000000002 0.34615 -0.00296]\n [1 0 1.0 ... 0.21378000000000003 0.18086 0.19083]] [0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1]\n(56, 34) (56,)\n"
    }
   ],
   "source": [
    "folds = 5 # delete folds later when embedded in function input\n",
    "# cv_train_data_ion is xTrain_ion split into five chunks\n",
    "dataset_split_in, cv_train_data_ion,cv_train_label_ion= cv.kfold_cross_validation(xTrain_ion,yTrain_ion,folds)\n",
    "# print(cv_train_data_ion,cv_train_label_ion)\n",
    "\n",
    "# the last input for cv.train_validation_split is the number of experiments you are running currently, 5 in total. \n",
    "# each experiments is organizing the five chunks from cv_train_data_ion into 4 chunks for training_data_ion and 1 chunk for validate_data_ion for cross validation, just need to uncomment the line you want to experiment currently. \n",
    "\n",
    "# will be in a for-loop when computing accuracy scores \n",
    "#exp1: \n",
    "validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,1)\n",
    "\n",
    "#exp2:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,2)\n",
    "\n",
    "#exp3:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,3)\n",
    "\n",
    "#exp4:\n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,4)\n",
    "\n",
    "#exp5: \n",
    "#validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,5)\n",
    "print(validate_data_ion,validate_labels_ion)\n",
    "print(validate_data_ion.shape,validate_labels_ion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Now fitting Ionosphere\n0.6139772778421112\n0.5976139123162385\n0.5818065165072646\n0.5665471347608791\n0.5518260786030147\n0.5376322266397486\n0.5239532964335114\n0.5107760884187924\n0.49808670261060606\n0.4858707293609807\n0.474113415762071\n0.4627998095078268\n0.45191488213578646\n0.4414436335999341\n0.4313711800947837\n0.4216828269768058\n0.4123641285257698\n0.4034009361665084\n0.3947794366395992\n0.38648618147397534\n0.37850810898031223\n0.37083255985455427\n0.36344728735846293\n0.35634046292995725\n0.3495006779710835\n0.34291694246594034\n0.3365786809947306\n0.33047572663297686\n0.3245983131563437\n0.31893706591085963\n0.31348299165498245\n0.3082274676332526\n0.30316223010056886\n0.29827936248078046\n0.29357128331272064\n0.2890307341104679\n0.2846507672419984\n0.28042473391103734\n0.2763462723104087\n0.2724092960011613\n0.26860798255986756\n0.26493676252649156\n0.2613903086768058\n0.25796352563632\n0.25465153984684014\n0.2514496898919516\n0.2483535171837555\n0.24535875700996193\n0.24246132993783662\n0.2396573335694233\n0.23694303464083166\n0.23431486145712144\n0.23176939665336266\n0.2293033702717747\n0.22691365314436643\n0.224597250570213\n0.22235129627635564\n0.22017304665128287\n0.21805987524001902\n0.21600926748998817\n0.21401881573702583\n0.2120862144211604\n0.2102092555220707\n0.20838582420443397\n0.20661389466371127\n0.2048915261632506\n0.2032168592539392\n0.20158811216797756\n0.20000357737870025\n0.19846161831870326\n0.19696066624887953\n0.19549921727128627\n0.19407582947908925\n0.19268912023713686\n0.19133776358701313\n0.19002048777070718\n0.18873607286731142\n0.18748334853742352\n0.1862611918701843\n0.18506852532811918\n0.18390431478518876\n0.18276756765366922\n0.18165733109569634\n0.1805726903155071\n0.17951276692860207\n0.17847671740423576\n0.17746373157781217\n0.17647303122993072\n0.17550386872898094\n0.17455552573433478\n0.17362731195732714\n0.17271856397734828\n0.17182864411050106\n0.17095693932839764\n0.17010286022478413\n0.16926584002779402\n0.16844533365573394\n0.16764081681440676\n0.1668517851340675\n0.16607775334420438\n0.1653182544844144\n0.16457283914973278\n0.16384107476884555\n0.1631225449136935\n0.16241684863904354\n0.1617235998506702\n0.1610424267008543\n0.16037297100996606\n0.15971488771295628\n0.1590678443296365\n0.15843152045767847\n0.15780560728731463\n0.15718980713676792\n0.15658383300748502\n0.15598740815828815\n0.15540026569760407\n0.1548221481929656\n0.1542528072970193\n0.1536920033893069\n0.15313950523312453\n0.15259508964679142\n0.1520585411886948\n0.15152965185550407\n0.15100822079297568\n0.1504940540187963\n0.1499869641569388\n0.14948677018302614\n0.14899329718022483\n0.14850637610520898\n0.1480258435637572\n0.14755154159556585\n0.14708331746787812\n0.1466210234775499\n0.14616451676118752\n0.14571365911301115\n0.14526831681011138\n0.14482836044478223\n0.14439366476362914\n0.14396410851316094\n0.14353957429159153\n0.14311994840658612\n0.14270512073870045\n0.14229498461027199\n0.14188943665953282\n0.14148837671972422\n0.1410917077030021\n0.1406993354889331\n0.14031116881738862\n0.13992711918565265\n0.1395471007495685\n0.13917103022855537\n0.13879882681433525\n0.1384304120832153\n0.13806570991177966\n0.13770464639584926\n0.13734714977257553\n0.1369931503455395\n0.13664258041273242\n0.1362953741973012\n0.13595146778094377\n0.1356107990398499\n0.1352733075830797\n0.13493893469328544\n0.1346076232696781\n0.13427931777315055\n0.13395396417346964\n0.133631509898454\n0.1333119037850577\n0.13299509603228427\n0.13268103815585763\n0.1323696829445797\n0.13206098441830785\n0.13175489778748783\n0.13145137941418156\n0.13115038677452884\n0.13085187842258852\n0.1305558139555033\n0.13026215397993757\n0.1299708600797373\n0.12968189478476508\n0.129395221540864\n0.12911080468090674\n0.12882860939688795\n0.12854860171301902\n0.12827074845978634\n0.1279950172489373\n0.12772137644935685\n0.12744979516380073\n0.1271802432064532\n0.12691269108127712\n0.12664710996112657\n0.12638347166759278\n0.12612174865155523\n0.12586191397441224\n0.12560394128996388\n0.12534780482692387\n0.12509347937203613\n0.12484094025377356\n0.12459016332659675\n0.12434112495575261\n0.12409380200259144\n0.1238481718103846\n0.12360421219062305\n0.12336190140977936\n0.12312121817651621\n0.12288214162932466\n0.12264465132457622\n0.12240872722497406\n0.12217434968838799\n0.12194149945705987\n0.12171015764716547\n0.12148030573871989\n0.1212519255658141\n0.12102499930717084\n0.12079950947700774\n0.12057543891619694\n0.1203527707837108\n0.12013148854834278\n0.11991157598069412\n0.11969301714541652\n0.119475796393702\n0.11925989835601082\n0.1190453079350292\n0.11883201029884843\n0.11861999087435843\n0.118409235340846\n0.1181997296237942\n0.1179914598888719\n0.11778441253610925\n0.11757857419425201\n0.11737393171528862\n0.11717047216914318\n0.11696818283852985\n0.11676705121396226\n0.1165670649889125\n0.11636821205511555\n0.11617048049801267\n0.11597385859233025\n0.11577833479778896\n0.1155838977549387\n0.11539053628111523\n0.11519823936651442\n0.11500699617038022\n0.1148167960173018\n0.11462762839361755\n0.11443948294392091\n0.11425234946766577\n0.11406621791586719\n0.1138810783878955\n0.11369692112835901\n0.11351373652407402\n0.11333151510111822\n0.11315024752196537\n0.11296992458269807\n0.11279053721029647\n0.11261207646000058\n0.11243453351274295\n0.11225789967265051\n0.11208216636461303\n0.11190732513191509\n0.11173336763393152\n0.11156028564388219\n0.1113880710466457\n0.11121671583662938\n0.11104621211569422\n0.11087655209113287\n0.11070772807369866\n0.11053973247568526\n0.11037255780905343\n0.1102061966836055\n0.11004064180520477\n0.1098758859740388\n0.10971192208292552\n0.10954874311566004\n0.10938634214540248\n0.1092247123331038\n0.10906384692596952\n0.10890373925596\n0.1087443827383265\n0.10858577087018088\n0.10842789722909933\n0.10827075547175827\n0.10811433933260162\n0.10795864262253871\n0.10780365922767188\n0.10764938310805229\n0.107495808296465\n0.10734292889723936\n0.1071907390850875\n0.10703923310396751\n0.10688840526597151\n0.10673824995023842\n0.10658876160188972\n0.10643993473098862\n0.10629176391152069\n0.1061442437803973\n0.10599736903647868\n0.10585113443961844\n0.10570553480972783\n0.10556056502585887\n0.10541622002530697\n0.10527249480273136\n0.10512938440929366\n0.10498688395181373\n0.10484498859194244\n0.10470369354535103\n0.1045629940809362\n0.10442288552004131\n0.10428336323569219\n0.10414442265184846\n0.10400605924266881\n0.10386826853179054\n0.10373104609162304\n0.10359438754265379\n0.10345828855276912\n0.10332274483658578\n0.10318775215479628\n0.10305330631352574\n0.10291940316370062\n0.10278603860042929\n0.10265320856239335\n0.10252090903125055\n0.10238913603104781\n0.10225788562764522\n0.10212715392814983\n0.10199693708035999\n0.10186723127221874\n0.10173803273127723\n0.10160933772416746\n0.1014811425560838\n0.10135344357027364\n0.10122623714753652\n0.10109951970573217\n0.10097328769929634\n0.10084753761876515\n0.10072226599030708\n0.10059746937526295\n0.1004731443696935\n0.10034928760393419\n0.1002258957421575\n0.10010296548194224\n0.0999804935538499\n"
    }
   ],
   "source": [
    "import Models.logisticRegression as lr\n",
    "\n",
    "ionslr1 = lr.Logistic_Regression(0.02,\"Ionosphere\",\"binary\") # input step size\n",
    "# params = ionslr1.fit(cv_train_data, cv_train_label, 0.02, 1e-1)\n",
    "params1 = ionslr1.fit(training_data_ion,training_labels_ion,0.02,1e-1)\n",
    "#adding a separate comment\n",
    "# input learning rate and termination conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1]\n[0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1]\n"
    }
   ],
   "source": [
    "# %% Making LR prediction on the validatoin set\n",
    "\n",
    "# need to convert predictions into 1 or 0\n",
    "\n",
    "# pred_vali1 is a boolean array, pred_vali*1 gives binary 1 or 0 automatically \n",
    "pred_vali = ionslr1.predict(params1,validate_data_ion)\n",
    "pred_vali=pred_vali * 1\n",
    "\n",
    "print(pred_vali)\n",
    "print(validate_labels_ion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
    }
   ],
   "source": [
    "# try prediction for testing set \n",
    "# print(xTest_ion); print(yTest_ion)\n",
    "pred_test = ionslr1.predict(params1,xTest_ion)\n",
    "# change boolean into 1 or 0\n",
    "pred_test = pred_test * 1\n",
    "print(pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "384947\n0.11332625892036158\n0.11310916630402876\n0.11289307394934439\n0.11267797475080919\n0.1124638616721036\n0.11225072774527053\n0.11203856606990936\n0.11182736981237903\n0.1116171322050119\n0.11140784654533795\n0.11119950619531771\n0.11099210458058618\n0.11078563518970568\n0.1105800915734283\n0.11037546734396772\n0.11017175617428045\n0.10996895179735618\n0.10976704800551688\n0.10956603864972514\n0.10936591763890115\n0.10916667893924839\n0.1089683165735881\n0.10877082462070162\n0.10857419721468183\n0.10837842854429235\n0.1081835128523351\n0.1079894444350258\n0.10779621764137728\n0.10760382687259133\n0.10741226658145704\n0.10722153127175786\n0.10703161549768589\n0.10684251386326328\n0.10665422102177156\n0.1064667316751876\n0.10628004057362708\n0.10609414251479515\n0.10590903234344383\n0.10572470495083622\n0.10554115527421762\n0.10535837829629373\n0.10517636904471461\n0.10499512259156606\n0.10481463405286703\n0.10463489858807319\n0.10445591139958743\n0.10427766773227573\n0.10410016287299004\n0.10392339215009645\n0.10374735093300977\n0.10357203463173383\n0.10339743869640769\n0.1032235586168573\n0.10305038992215348\n0.10287792818017454\n0.10270616899717543\n0.10253510801736139\n0.10236474092246767\n0.10219506343134432\n0.10202607129954613\n0.10185776031892765\n0.1016901263172438\n0.10152316515775488\n0.10135687273883694\n0.10119124499359695\n0.10102627788949287\n0.10086196742795825\n0.100698309644032\n0.1005353006059922\n0.10037293641499517\n0.10021121320471842\n0.10005012714100876\n0.09988967442153399\nNow fitting Ionosphere\n0.19809560475298027\n0.19745020199171684\n0.19680835528862645\n0.19617003875440114\n0.19553522670613163\n0.19490389366683036\n0.1942760143649035\n0.19365156373357353\n0.1930305169102559\n0.19241284923588875\n0.19179853625422022\n0.19118755371105478\n0.19057987755345857\n0.1899754839289278\n0.18937434918452056\n0.1887764498659533\n0.1881817627166658\n0.1875902646768521\n0.18700193288246425\n0.18641674466418506\n0.18583467754637553\n0.18525570924599458\n0.18467981767149624\n0.1841069809217025\n0.18353717728465443\n0.18297038523644293\n0.18240658344001975\n0.18184575074399037\n0.181287866181389\n0.1807329089684386\n0.18018085850329488\n0.17963169436477605\n0.17908539631108092\n0.17854194427849301\n0.1780013183800747\n0.17746349890435079\n0.176928466313982\n0.17639620124443117\n0.17586668450262027\n0.17533989706558128\n0.1748158200791006\n0.17429443485635762\n0.17377572287655943\n0.17325966578357016\n0.1727462453845379\n0.17223544364851798\n0.17172724270509512\n0.17122162484300263\n0.17071857250874195\n0.17021806830520073\n0.1697200949902712\n0.1692246354754697\n0.16873167282455645\n0.1682411902521574\n0.16775317112238788\n0.16726759894747883\n0.16678445738640582\n0.16630373024352132\n0.16582540146719082\n0.16534945514843263\n0.1648758755195625\n0.1644046469528431\n0.16393575395913745\n0.16346918118656947\n0.16300491341918893\n0.16254293557564273\n0.16208323270785266\n0.16162578999969965\n0.16117059276571447\n0.16071762644977552\n0.16026687662381434\n0.15981832898652804\n0.15937196936209955\n0.15892778369892563\n0.15848575806835327\n0.15804587866342396\n0.15760813179762623\n0.15717250390365717\n0.15673898153219212\n0.15630755135066368\n0.155878200142049\n0.15545091480366696\n0.15502568234598366\n0.15460248989142786\n0.15418132467321516\n0.15376217403418233\n0.1533450254256307\n0.15292986640617914\n0.152516684640627\n0.15210546789882642\n0.15169620405456496\n0.1512888810844576\n0.15088348706684865\n0.15048001018072388\n0.15007843870463256\n0.14967876101561875\n0.14928096558816412\n0.14888504099313898\n0.14849097589676483\n0.14809875905958614\n0.14770837933545225\n0.1473198256705096\n0.1469330871022039\n0.14654815275829214\n0.14616501185586464\n0.14578365370037744\n0.14540406768469444\n0.14502624328813912\n0.1446501700755571\n0.14427583769638755\n0.14390323588374535\n0.14353235445351256\n0.14316318330343988\n0.14279571241225772\n0.14242993183879746\n0.14206583172112155\n0.14170340227566403\n0.1413426337963801\n0.14098351665390557\n0.14062604129472558\n0.1402701982403527\n0.13991597808651451\n0.13956337150235043\n0.13921236922961788\n0.1388629620819073\n0.1385151409438667\n0.13816889677043503\n0.1378242205860847\n0.13748110348407264\n0.1371395366257006\n0.1367995112395841\n0.13646101862093016\n0.13612405013082354\n0.13578859719552155\n0.1354546513057575\n0.1351222040160526\n0.13479124694403571\n0.13446177176977245\n0.13413377023510106\n0.1338072341429777\n0.13348215535682914\n0.13315852579991344\n0.13283633745468867\n0.13251558236218938\n0.1321962526214111\n0.13187834038870191\n0.1315618378771623\n0.1312467373560518\n0.13093303115020394\n0.13062071163944758\n0.1303097712580365\n0.1300002024940853\n0.1296919978890132\n0.12938515003699433\n0.12907965158441512\n0.12877549522933887\n0.12847267372097695\n0.12817117985916643\n0.12787100649385508\n0.12757214652459226\n0.1272745929000267\n0.12697833861741076\n0.12668337672211066\n0.12638970030712354\n0.12609730251260035\n0.1258061765253749\n0.12551631557849946\n0.12522771295078516\n0.12494036196635018\n0.12465425599417178\n0.1243693884476454\n0.12408575278414934\n0.12380334250461451\n0.1235221511531\n0.12324217231637458\n0.1229633996235028\n0.12268582674543738\n0.12240944739461594\n0.12213425532456351\n0.12186024432950027\n0.12158740824395406\n0.12131574094237785\n0.12104523633877254\n0.12077588838631431\n0.120507691076987\n0.12024063844121899\n0.11997472454752518\n0.11970994350215304\n0.1194462894487338\n0.11918375656793762\n0.11892233907713368\n0.1186620312300545\n0.11840282731646413\n0.1181447216618312\n0.11788770862700616\n0.11763178260790223\n0.11737693803518061\n0.11712316937394\n0.11687047112340981\n0.11661883781664724\n0.11636826402023856\n0.11611874433400386\n0.11587027339070562\n0.11562284585576131\n0.11537645642695912\n0.11513109983417769\n0.11488677083910921\n0.11464346423498611\n0.11440117484631125\n0.11415989752859106\n0.11391962716807277\n0.11368035868148452\n0.11344208701577843\n0.1132048071478775\n0.1129685140844255\n0.11273320286153943\n0.11249886854456588\n0.11226550622783983\n0.11203311103444663\n0.11180167811598689\n0.11157120265234449\n0.11134167985145678\n0.11111310494908858\n0.11088547320860807\n0.11065877992076581\n0.11043302040347644\n0.11020819000160312\n0.10998428408674407\n0.10976129805702249\n0.10953922733687829\n0.10931806737686268\n0.10909781365343535\n0.10887846166876339\n0.10866000695052366\n0.10844244505170653\n0.10822577155042243\n0.10800998204971081\n0.10779507217735074\n0.10758103758567439\n0.10736787395138245\n0.10715557697536146\n0.10694414238250362\n0.10673356592152845\n0.10652384336480683\n0.10631497050818636\n0.10610694317081955\n0.10589975719499363\n0.10569340844596194\n0.1054878928117779\n0.10528320620313028\n0.10507934455318066\n0.10487630381740273\n0.10467407997342294\n0.1044726690208635\n0.1042720669811868\n0.10407226989754165\n0.10387327383461101\n0.1036750748784618\n0.10347766913639586\n0.10328105273680292\n0.10308522182901503\n0.1028901725831623\n0.10269590119003072\n0.102502403860921\n0.1023096768275093\n0.10211771634170905\n0.10192651867553454\n0.10173608012096579\n0.1015463969898151\n0.10135746561359438\n0.10116928234338447\n0.10098184354970567\n0.1007951456223893\n0.10060918497045095\n0.1004239580219648\n0.10023946122393929\n0.10005569104219404\n0.09987264396123798\nNow fitting Ionosphere\n0.21927324446862168\n0.21859902341775583\n0.21792798496371416\n0.2172601121053248\n0.21659538794571936\n0.21593379569182358\n0.2152753186538353\n0.21461994024469067\n0.21396764397951995\n0.21331841347509242\n0.2126722324492521\n0.21202908472034399\n0.21138895420663228\n0.21075182492571062\n0.21011768099390524\n0.20948650662567156\n0.20885828613298443\n0.2082330039247227\n0.2076106445060493\n0.20699119247778586\n0.20637463253578414\n0.20576094947029322\n0.205150128165323\n0.2045421535980063\n0.20393701083795693\n0.2033346850466266\n0.2027351614766606\n0.20213842547125088\n0.20154446246348934\n0.20095325797571975\n0.2003647976188901\n0.1997790670919043\n0.19919605218097441\n0.19861573875897384\n0.19803811278479025\n0.19746316030268102\n0.19689086744162923\n0.19632122041470118\n0.19575420551840605\n0.1951898091320574\n0.19462801771713703\n0.194068817816661\n0.1935121960545487\n0.19295813913499427\n0.19240663384184098\n0.19185766703795937\n0.1913112256646274\n0.19076729674091575\n0.19022586736307479\n0.18968692470392662\n0.18915045601225994\n0.18861644861222956\n0.18808488990275962\n0.1875557673569505\n0.18702906852149062\n0.18650478101607212\n0.1859828925328106\n0.1854633908356703\n0.18494626375989245\n0.18443149921142918\n0.18391908516638145\n0.1834090096704426\n0.18290126083834463\n0.1823958268533118\n0.181892695966517\n0.18139185649654382\n0.18089329682885363\n0.18039700541525663\n0.17990297077338918\n0.17941118148619484\n0.1789216262014108\n0.17843429363105956\n0.1779491725509447\n0.17746625180015302\n0.17698552028056005\n0.17650696695634155\n0.17603058085348966\n0.175556351059334\n0.17508426672206787\n0.17461431705027872\n0.17414649131248477\n0.1736807788366749\n0.1732171690098552\n0.17275565127759884\n0.17229621514360147\n0.17183885016924158\n0.17138354597314517\n0.17093029223075556\n0.17047907867390774\n0.1700298950904071\n0.16958273132361384\n0.16913757727203063\n0.16869442288889597\n0.1682532581817813\n0.1678140732121934\n0.16737685809518055\n0.16694160299894367\n0.16650829814445153\n0.16607693380506028\n0.16564750030613756\n0.16521998802469093\n0.16479438738899999\n0.16437068887825337\n0.16394888302218943\n0.16352896040074108\n0.16311091164368507\n0.16269472743029445\n0.1622803984889961\n0.1618679155970313\n0.16145726958012036\n0.16104845131213144\n0.16064145171475294\n0.16023626175716935\n0.15983287245574093\n0.1594312748736878\n0.15903146012077599\n0.15863341935300893\n0.15823714377232082\n0.15784262462627466\n0.15744985320776275\n0.15705882085471143\n0.15666951894978826\n0.1562819389201133\n0.15589607223697316\n0.15551191041553789\n0.15512944501458203\n0.15474866763620776\n0.15436956992557158\n0.15399214357061397\n0.15361638030179187\n0.15324227189181405\n0.15286981015537957\n0.1524989869489191\n0.1521297941703384\n0.15176222375876555\n0.15139626769430015\n0.15103191799776505\n0.15066916673046163\n0.1503080059939269\n0.14994842792969282\n0.14959042471904924\n0.14923398858280842\n0.14887911178107222\n0.14852578661300164\n0.14817400541658896\n0.1478237605684317\n0.14747504448350932\n0.14712784961496192\n0.1467821684538712\n0.14643799352904355\n0.1460953174067954\n0.1457541326907404\n0.1454144320215789\n0.14507620807688937\n0.14473945357092197\n0.14440416125439362\n0.14407032391428568\n0.14373793437364288\n0.1434069854913745\n0.14307747016205755\n0.1427493813157409\n0.14242271191775224\n0.14209745496850648\n0.1417736035033151\n0.1414511505921984\n0.14113008933969853\n0.14081041288469445\n0.14049211440021858\n0.14017518709327487\n0.13985962420465867\n0.13954541900877765\n0.13923256481347468\n0.1389210549598522\n0.13861088282209763\n0.1383020418073108\n0.13799452535533174\n0.1376883269385715\n0.1373834400618429\n0.13707985826219304\n0.13677757510873736\n0.13647658420249503\n0.1361768791762251\n0.13587845369426424\n0.1355813014523661\n0.1352854161775411\n0.13499079162789807\n0.13469742159248657\n0.13440529989114075\n0.1341144203743244\n0.13382477692297642\n0.13353636344835865\n0.13324917389190313\n0.1329632022250622\n0.1326784424491582\n0.1323948885952351\n0.1321125347239109\n0.13183137492523075\n0.13155140331852153\n0.1312726140522469\n0.13099500130386416\n0.13071855927968068\n0.1304432822147127\n0.13016916437254444\n0.12989620004518748\n0.12962438355294234\n0.12935370924425993\n0.12908417149560417\n0.12881576471131556\n0.12854848332347552\n0.12828232179177154\n0.12801727460336318\n0.12775333627274887\n0.12749050134163342\n0.12722876437879665\n0.12696811997996213\n0.12670856276766754\n0.12645008739113506\n0.1261926885261431\n0.12593636087489785\n0.125681099165907\n0.12542689815385272\n0.12517375261946606\n0.12492165736940238\n0.12467060723611648\n0.12442059707773954\n0.12417162177795556\n0.12392367624587972\n0.12367675541593641\n0.12343085424773823\n0.1231859677259659\n0.12294209086024847\n0.1226992186850442\n0.12245734625952212\n0.12221646866744439\n0.12197658101704885\n0.1217376784409327\n0.12149975609593619\n0.1212628091630273\n0.12102683284718703\n0.12079182237729495\n0.12055777300601554\n0.12032468000968521\n0.12009253868819948\n0.11986134436490127\n0.11963109238646906\n0.11940177812280635\n0.11917339696693093\n0.1189459443348653\n0.11871941566552713\n0.11849380642062064\n0.11826911208452824\n0.1180453281642026\n0.11782245018905979\n0.11760047371087205\n0.11737939430366201\n0.11715920756359642\n0.11693990910888144\n0.11672149457965725\n0.11650395963789456\n0.11628729996728994\n0.11607151127316308\n0.11585658928235382\n0.11564252974311956\n0.11542932842503349\n0.11521698111888332\n0.11500548363656986\n0.11479483181100683\n0.11458502149602075\n0.114376048566251\n0.11416790891705102\n0.11396059846438944\n0.11375411314475165\n0.11354844891504212\n0.11334360175248676\n0.11313956765453621\n0.11293634263876905\n0.11273392274279578\n0.11253230402416299\n0.11233148256025825\n0.11213145444821514\n0.11193221580481877\n0.1117337627664117\n0.11153609148880067\n0.1113391981471629\n0.11114307893595343\n0.11094773006881312\n0.11075314777847599\n0.11055932831667814\n0.11036626795406641\n0.1101739629801075\n0.1099824097029976\n0.1097916044495724\n0.10960154356521763\n0.10941222341377947\n0.10922364037747609\n0.10903579085680894\n0.10884867127047476\n0.10866227805527792\n0.10847660766604306\n0.10829165657552818\n0.10810742127433812\n0.1079238982708383\n0.10774108409106892\n0.10755897527865964\n0.10737756839474431\n0.10719686001787655\n0.10701684674394508\n0.10683752518609002\n0.10665889197461914\n0.10648094375692475\n0.10630367719740062\n0.10612708897735967\n0.10595117579495157\n0.10577593436508118\n0.10560136141932697\n0.10542745370585976\n0.10525420798936233\n0.10508162105094866\n0.10490968968808406\n0.10473841071450549\n0.10456778096014205\n0.10439779727103606\n0.10422845650926447\n0.10405975555286032\n0.1038916912957349\n0.10372426064760014\n0.10355746053389107\n0.10339128789568909\n0.10322573968964524\n0.10306081288790381\n0.1028965044780266\n0.1027328114629169\n0.10256973086074454\n0.10240725970487076\n0.10224539504377349\n0.1020841339409732\n0.1019234734749587\n0.1017634107391136\n0.10160394284164295\n0.1014450669055001\n0.10128678006831408\n0.10112907948231728\n0.10097196231427318\n0.10081542574540486\n0.10065946697132323\n0.10050408320195624\n0.10034927166147783\n0.1001950295882377\n0.10004135423469081\n0.09988824286732766\nNow fitting Ionosphere\n0.2192277945262807\n0.21836331421947733\n0.21750520767629367\n0.2166534146376578\n0.21580787533268206\n0.21496853048189996\n0.21413532130012747\n0.2133081894989662\n0.2124870772889622\n0.21167192738143303\n0.210862682989982\n0.21005928783170721\n0.20926168612812265\n0.2084698226058024\n0.20768364249676005\n0.20690309153857553\n0.20612811597428085\n0.2053586625520158\n0.20459467852446378\n0.20383611164807816\n0.203082910182111\n0.20233502288744945\n0.20159239902527412\n0.20085498835554497\n0.20012274113532424\n0.19939560811694512\n0.1986735405460342\n0.19795649015939432\n0.1972444091827566\n0.1965372503284085\n0.19583496679270382\n0.19513751225346268\n0.1944448408672665\n0.19375690726665618\n0.19307366655723598\n0.19239507431469313\n0.19172108658173426\n0.19105165986494774\n0.19038675113159387\n0.18972631780633015\n0.1890703177678745\n0.1884187093456122\n0.18777145131614964\n0.18712850289982072\n0.18648982375714762\n0.1858553739852617\n0.18522511411428672\n0.1845990051036892\n0.18397700833859829\n0.18335908562609723\n0.182745199191494\n0.18213531167456712\n0.18152938612579597\n0.18092738600257416\n0.18032927516541028\n0.17973501787411822\n0.17914457878399861\n0.17855792294201484\n0.17797501578296485\n0.1773958231256512\n0.1768203111690513\n0.1762484464884891\n0.17568019603181123\n0.17511552711556744\n0.17455440742119885\n0.1739968049912346\n0.17344268822549763\n0.17289202587732277\n0.17234478704978642\n0.17180094119195086\n0.17126045809512266\n0.17072330788912823\n0.17018946103860544\n0.16965888833931378\n0.16913156091446374\n0.16860745021106588\n0.16808652799630064\n0.16756876635390952\n0.16705413768060917\n0.16654261468252762\n0.16603417037166474\n0.1655287780623761\n0.1650264113678828\n0.16452704419680556\n0.16403065074972495\n0.16353720551576795\n0.1630466832692211\n0.16255905906617057\n0.16207430824117\n0.1615924064039354\n0.16111332943606888\n0.16063705348781016\n0.16016355497481702\n0.15969281057497378\n0.15922479722522967\n0.15875949211846602\n0.15829687270039214\n0.1578369166664717\n0.1573796019588778\n0.1569249067634779\n0.15647280950684875\n0.15602328885332067\n0.1555763237020518\n0.15513189318413223\n0.1546899766597178\n0.15425055371519392\n0.15381360416036874\n0.15337910802569715\n0.15294704555953278\n0.15251739722541152\n0.15209014369936322\n0.15166526586725348\n0.1512427448221552\n0.15082256186174853\n0.15040469848575083\n0.14998913639337533\n0.14957585748081892\n0.14916484383877854\n0.14875607774999672\n0.14834954168683476\n0.14794521830887536\n0.14754309046055286\n0.14714314116881116\n0.1467453536407905\n0.14634971126154106\n0.14595619759176442\n0.14556479636558228\n0.14517549148833225\n0.1447882670343906\n0.14440310724502187\n0.14401999652625444\n0.1436389194467835\n0.14325986073589853\n0.1428828052814382\n0.14250773812777004\n0.14213464447379567\n0.14176350967098147\n0.14139431922141427\n0.14102705877588123\n0.14066171413197523\n0.14029827123222352\n0.13993671616224143\n0.13957703514890904\n0.13921921455857256\n0.13886324089526753\n0.13850910079896703\n0.1381567810438513\n0.1378062685366009\n0.13745755031471196\n0.13711061354483386\n0.1367654455211288\n0.13642203366365305\n0.13608036551675984\n0.13574042874752362\n0.13540221114418527\n0.13506570061461795\n0.13473088518481394\n0.1343977529973915\n0.13406629231012213\n0.1337364914944775\n0.13340833903419622\n0.13308182352386996\n0.1327569336675489\n0.13243365827736595\n0.13211198627218004\n0.1317919066762378\n0.13147340861785356\n0.13115648132810734\n0.13084111413956107\n0.1305272964849921\n0.13021501789614434\n0.12990426800249674\n0.12959503653004856\n0.12928731330012144\n0.12898108822817847\n0.12867635132265906\n0.1283730926838305\n0.128071302502655\n0.12777097105967336\n0.12747208872390292\n0.12717464595175193\n0.12687863328594884\n0.12658404135448623\n0.1262908608695799\n0.12599908262664236\n0.1257086975032705\n0.12541969645824816\n0.12513207053056155\n0.12484581083842992\n0.12456090857834855\n0.12427735502414605\n0.12399514152605445\n0.12371425950979269\n0.12343470047566289\n0.1231564559976592\n0.12287951772258962\n0.12260387736920988\n0.12232952672736938\n0.12205645765716994\n0.12178466208813525\n0.12151413201839331\n0.12124485951386944\n0.12097683670749115\n0.12071005579840419\n0.12044450905119977\n0.12018018879515206\n0.11991708742346766\n0.11965519739254438\n0.11939451122124162\n0.11913502149015988\n0.11887672084093161\n0.11861960197552146\n0.11836365765553625\n0.11810888070154525\n0.11785526399240931\n0.11760280046462032\n0.11735148311164922\n0.11710130498330361\n0.11685225918509419\n0.11660433887761047\n0.1163575372759048\n0.11611184764888528\n0.11586726331871745\n0.11562377766023381\n0.11538138410035235\n0.11514007611750245\n0.11489984724105949\n0.11466069105078718\n0.11442260117628734\n0.11418557129645786\n0.11394959513895803\n0.11371466647968145\n0.1134807791422359\n0.113247926997431\n0.11301610396277272\n0.11278530400196474\n0.11255552112441727\n0.11232674938476206\n0.11209898288237456\n0.11187221576090262\n0.11164644220780143\n0.11142165645387535\n0.11119785277282578\n0.11097502548080501\n0.11075316893597688\n0.11053227753808265\n0.11031234572801375\n0.11009336798738938\n0.10987533883814067\n0.1096582528421003\n0.10944210460059735\n0.10922688875405828\n0.10901259998161285\n0.1087992330007055\n0.10858678256671224\n0.10837524347256229\n0.10816461054836543\n0.10795487866104352\n0.1077460427139679\n0.10753809764660073\n0.10733103843414223\n0.10712486008718167\n0.10691955765135304\n0.10671512620699655\n0.10651156086882287\n0.10630885678558297\n0.10610700913974219\n0.10590601314715825\n0.10570586405676388\n0.10550655715025312\n0.1053080877417726\n0.10511045117761587\n0.10491364283592247\n0.10471765812638037\n0.10452249248993291\n0.10432814139848891\n0.10413460035463677\n0.10394186489136242\n0.10374993057177082\n0.1035587929888109\n0.10336844776500394\n0.10317889055217595\n0.10299011703119286\n0.10280212291169923\n0.10261490393186067\n0.10242845585810884\n0.10224277448489048\n0.10205785563441862\n0.10187369515642777\n0.10169028892793182\n0.10150763285298481\n0.10132572286244496\n0.1011445549137414\n0.10096412499064406\n0.10078442910303605\n0.10060546328668898\n0.10042722360304125\n0.10024970613897854\n0.1000729070066177\n0.09989682234309243\nLR accuracy score: 0.7178571428571429\nLR precision score: 0.6990436174469788\nLR recall score: 0.9855927396483267\nLR f1 score: 0.814195913450156\n"
    }
   ],
   "source": [
    "\n",
    "# %% Using evaluate_acc to do 5-fold CV\n",
    "\n",
    "# LOGISTIC REGRESSION MODEL \n",
    "# APPEND SCORES\n",
    "vali_acc_score_lr=[]; vali_preci_socre_lr=[]; vali_recall_score_lr=[];vali_f1_score_lr=[]\n",
    "# implement LR model \n",
    "import Models.logisticRegression as lr\n",
    "import Models.evaluate_acc as acc\n",
    "# LogReg1 =LogisticRegression()\n",
    "\n",
    "for i in range(5):\n",
    "    validate_data_ion,validate_labels_ion,training_data_ion,training_labels_ion = cv.train_validation_split(cv_train_data_ion,cv_train_label_ion,i+1)\n",
    "    training_data_ion=training_data_ion.astype('int'); training_labels_ion=training_labels_ion.astype('int')\n",
    "    validate_data_ion=validate_data_ion.astype('int'); validate_labels_ion=validate_labels_ion.astype('int')\n",
    "    \n",
    "    # delete the previous loop's model \n",
    "    # if i > 0:\n",
    "    #    del LogReg1\n",
    " \n",
    "    ionslr1 = lr.Logistic_Regression(0.02,\"Ionosphere\",\"binary\")                # build model\n",
    "    params1 = ionslr1.fit(training_data_ion,training_labels_ion,0.02,1e-1)      # find parameters to predict \n",
    "    validate_pred_lr= ionslr1.predict(params1,validate_data_ion)                # predict on validation data \n",
    "    validate_pred_lr=validate_pred_lr * 1                                       # change True of False from validate_pred_lr into binary \n",
    "    \n",
    "    # accuracy score \n",
    "    validate_expect_lr = validate_labels_ion\n",
    "    tp, tn, fn, fp = acc.compute_tp_tn_fn_fp(validate_expect_lr,validate_pred_lr) # for other score computation \n",
    "    conf_mat = acc.compute_tp_tn_fn_fp(validate_expect_lr,validate_pred_lr)     # confusion matrix: (tp, tn, fn, fp)\n",
    "    validate_acc_score_lr = (acc.compute_accuracy(*list(conf_mat)))*0.01        # compute accuracy score   \n",
    "    vali_acc_score_lr.append(validate_acc_score_lr)                             # append validation accuracy score for each experiment \n",
    "    \n",
    "    # precision score \n",
    "    validate_preci_score_lr = acc.compute_precision(tp, fp)*0.01\n",
    "    vali_preci_socre_lr.append(validate_preci_score_lr)    \n",
    "\n",
    "    # recall score \n",
    "    validate_recall_score_lr = acc.compute_recall(tp, fn)*0.01\n",
    "    vali_recall_score_lr.append(validate_recall_score_lr)\n",
    "    \n",
    "    # f1 score \n",
    "    validate_f1_score_lr = acc.compute_f1_score(validate_expect_lr,validate_pred_lr)\n",
    "    vali_f1_score_lr.append(validate_f1_score_lr)\n",
    "    \n",
    "\n",
    "\n",
    "mu_vali_acc_lr=np.mean(vali_acc_score_lr); \n",
    "mu_vali_preci_lr=np.mean(vali_preci_socre_lr); \n",
    "mu_vali_recall_lr=np.mean(vali_recall_score_lr);\n",
    "mu_vali_f1_lr=np.mean(vali_f1_score_lr)\n",
    "print('LR accuracy score:',mu_vali_acc_lr)\n",
    "print('LR precision score:',mu_vali_preci_lr)\n",
    "print('LR recall score:',mu_vali_recall_lr)\n",
    "print('LR f1 score:',mu_vali_f1_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}